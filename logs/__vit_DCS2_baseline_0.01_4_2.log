nohup: ignoring input
wandb: Currently logged in as: xuanzhu_07 (xuanzhu_07-university-of-sydney). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.19.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/yxma/hzx/hzx/rand_defence/wandb/run-20250328_002116-zgs4ar3e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-aardvark-206
wandb: ⭐️ View project at https://wandb.ai/xuanzhu_07-university-of-sydney/findVitVit
wandb: 🚀 View run at https://wandb.ai/xuanzhu_07-university-of-sydney/findVitVit/runs/zgs4ar3e
[2025/03/28 00:21:21] - Namespace(batch_size=128, data_dir='~/datasets/CIFAR/', dataset='cifar10', epochs=200, network='earlyVit', worker=4, lr_schedule='multistep', lr_min=0.0, lr_max=0.01, weight_decay=0.0005, momentum=0.9, none_random_training=True, rand_deform_training=False, randpos_deform_training=False, randpos_multi_deform_training=False, only_adv_randpos_training=False, rand_path_training=False, layerNum=2, randType='repeated', epsilon=8, alpha=2, seed=0, attack_iters=7, restarts=1, none_adv_training=False, save_dir='ckpt/cifar10/earlyVit/ckpt', pretrain='/home/yxma/hzx/hzx/rand_defence/ckpt/cifar10/earlyVit/ckpt/model_20250327205500_27e.pth', continue_training=True, lb=2048, pos=0, eot=False, hang=False, device=1, optimizer='sgd', num_classes=10)
[2025/03/28 00:21:22] - EarlyConvViT(
  (conv_layers): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (1): Sequential(
      (0): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (2): Sequential(
      (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (3): Sequential(
      (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (conv_1x1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
    (flatten image): Rearrange('batch channels height width -> batch (height width) channels')
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (transformer): Transformer(
    (layers): ModuleList(
      (0-10): 11 x ModuleList(
        (0): PreNorm(
          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (fn): Attention(
            (attend): Softmax(dim=-1)
            (to_qkv): Linear(in_features=192, out_features=576, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=192, out_features=192, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): PreNorm(
          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (fn): FeedForward(
            (net): Sequential(
              (0): Linear(in_features=192, out_features=576, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=576, out_features=192, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
  )
  (to_latent): Identity()
  (mlp_head): Sequential(
    (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=192, out_features=10, bias=True)
  )
)
/home/yxma/anaconda3/envs/py39_torch/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
[2025/03/28 00:21:22] - epoch 27 lr 1.000000e-02
start_time 20250328002121
Process ID: 1097356
Files already downloaded and verified
Files already downloaded and verified
303030


 Resume from Epoch 27. Load pretrained weight.
Best PGD ACC 0.307700.
Best Natural ACC 0.482700.
Training using fixed weight.


[2025/03/28 00:21:24] - Iter: [27][0/391]	Loss 1.773 (1.773)	Prec@1 0.367 (0.367)	
[2025/03/28 00:21:54] - Iter: [27][50/391]	Loss 1.874 (1.844)	Prec@1 0.289 (0.309)	
[2025/03/28 00:22:23] - Iter: [27][100/391]	Loss 1.881 (1.850)	Prec@1 0.297 (0.306)	
[2025/03/28 00:22:52] - Iter: [27][150/391]	Loss 2.014 (1.859)	Prec@1 0.234 (0.304)	
[2025/03/28 00:23:21] - Iter: [27][200/391]	Loss 1.876 (1.864)	Prec@1 0.281 (0.302)	
[2025/03/28 00:23:50] - Iter: [27][250/391]	Loss 1.908 (1.866)	Prec@1 0.266 (0.302)	
[2025/03/28 00:24:19] - Iter: [27][300/391]	Loss 1.752 (1.864)	Prec@1 0.344 (0.303)	
[2025/03/28 00:24:48] - Iter: [27][350/391]	Loss 1.887 (1.863)	Prec@1 0.266 (0.303)	
[2025/03/28 00:25:11] - Evaluating with standard images...
[2025/03/28 00:25:14] - Evaluating with PGD Attack...
[2025/03/28 00:27:12] - Test Loss: 1.5286  	 Test Acc: 0.4616  
 PGD Loss: 1.8696 	 PGD Acc: 0.3043 
 Best PGD Acc: 0.3077 	 Test Acc of best PGD ckpt: 0.4827
[2025/03/28 00:27:12] - epoch 28 lr 1.000000e-02
[2025/03/28 00:27:13] - Iter: [28][0/391]	Loss 1.854 (1.854)	Prec@1 0.305 (0.305)	
[2025/03/28 00:27:42] - Iter: [28][50/391]	Loss 1.928 (1.859)	Prec@1 0.297 (0.296)	
[2025/03/28 00:28:11] - Iter: [28][100/391]	Loss 1.997 (1.860)	Prec@1 0.258 (0.301)	
[2025/03/28 00:28:40] - Iter: [28][150/391]	Loss 1.693 (1.859)	Prec@1 0.406 (0.301)	
[2025/03/28 00:29:09] - Iter: [28][200/391]	Loss 1.906 (1.857)	Prec@1 0.312 (0.303)	
[2025/03/28 00:29:38] - Iter: [28][250/391]	Loss 1.876 (1.858)	Prec@1 0.289 (0.302)	
[2025/03/28 00:30:07] - Iter: [28][300/391]	Loss 1.782 (1.860)	Prec@1 0.320 (0.303)	
[2025/03/28 00:30:36] - Iter: [28][350/391]	Loss 1.871 (1.859)	Prec@1 0.352 (0.303)	
[2025/03/28 00:30:59] - Evaluating with standard images...
[2025/03/28 00:31:02] - Evaluating with PGD Attack...
[2025/03/28 00:33:00] - Test Loss: 1.5072  	 Test Acc: 0.4751  
 PGD Loss: 1.8755 	 PGD Acc: 0.2988 
 Best PGD Acc: 0.3077 	 Test Acc of best PGD ckpt: 0.4827
[2025/03/28 00:33:00] - epoch 29 lr 1.000000e-02
[2025/03/28 00:33:01] - Iter: [29][0/391]	Loss 1.909 (1.909)	Prec@1 0.289 (0.289)	
[2025/03/28 00:33:30] - Iter: [29][50/391]	Loss 1.776 (1.861)	Prec@1 0.328 (0.297)	
[2025/03/28 00:33:59] - Iter: [29][100/391]	Loss 1.813 (1.860)	Prec@1 0.344 (0.300)	
[2025/03/28 00:34:28] - Iter: [29][150/391]	Loss 1.944 (1.858)	Prec@1 0.273 (0.302)	
[2025/03/28 00:34:57] - Iter: [29][200/391]	Loss 1.925 (1.856)	Prec@1 0.328 (0.302)	
[2025/03/28 00:35:26] - Iter: [29][250/391]	Loss 1.804 (1.857)	Prec@1 0.305 (0.302)	
[2025/03/28 00:35:55] - Iter: [29][300/391]	Loss 1.786 (1.858)	Prec@1 0.352 (0.302)	
[2025/03/28 00:36:24] - Iter: [29][350/391]	Loss 1.968 (1.858)	Prec@1 0.219 (0.302)	
[2025/03/28 00:36:47] - Evaluating with standard images...
[2025/03/28 00:36:50] - Evaluating with PGD Attack...
[2025/03/28 00:38:48] - Test Loss: 1.5128  	 Test Acc: 0.4544  
 PGD Loss: 1.8909 	 PGD Acc: 0.2932 
 Best PGD Acc: 0.3077 	 Test Acc of best PGD ckpt: 0.4827
[2025/03/28 00:38:48] - epoch 30 lr 1.000000e-02
[2025/03/28 00:38:49] - Iter: [30][0/391]	Loss 1.855 (1.855)	Prec@1 0.375 (0.375)	
[2025/03/28 00:39:18] - Iter: [30][50/391]	Loss 1.826 (1.871)	Prec@1 0.305 (0.294)	
[2025/03/28 00:39:47] - Iter: [30][100/391]	Loss 1.862 (1.867)	Prec@1 0.344 (0.298)	
