nohup: ignoring input
wandb: Currently logged in as: xuanzhu_07 (xuanzhu_07-university-of-sydney). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/yxma/hzx/hzx/rand_defence/wandb/run-20250326_020025-n8alyz1a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-night-149
wandb: ⭐️ View project at https://wandb.ai/xuanzhu_07-university-of-sydney/findVitVit
wandb: 🚀 View run at https://wandb.ai/xuanzhu_07-university-of-sydney/findVitVit/runs/n8alyz1a
[2025/03/26 02:00:28] - Namespace(batch_size=128, data_dir='~/datasets/CIFAR/', dataset='cifar10', epochs=200, network='earlyVit', worker=4, lr_schedule='multistep', lr_min=0.0, lr_max=0.1, weight_decay=0.0005, momentum=0.9, none_random_training=True, rand_deform_training=False, randpos_deform_training=True, randpos_multi_deform_training=False, only_adv_randpos_training=False, rand_path_training=False, layerNum=2, randType='repeated', epsilon=8, alpha=2, seed=0, attack_iters=7, restarts=1, none_adv_training=False, save_dir='ckpt/cifar10/earlyVit/ckpt', pretrain=None, continue_training=False, lb=2048, pos=0, eot=False, hang=False, device=3, num_classes=10)
[2025/03/26 02:00:30] - EarlyConvViT(
  (conv_layers): Sequential(
    (0): Sequential(
      (0): MaskedConv2d(3, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))
    )
    (1): Sequential(
      (0): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (2): Sequential(
      (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (3): Sequential(
      (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (conv_1x1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
    (flatten image): Rearrange('batch channels height width -> batch (height width) channels')
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (transformer): Transformer(
    (layers): ModuleList(
      (0-10): 11 x ModuleList(
        (0): PreNorm(
          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (fn): Attention(
            (attend): Softmax(dim=-1)
            (to_qkv): Linear(in_features=192, out_features=576, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=192, out_features=192, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): PreNorm(
          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (fn): FeedForward(
            (net): Sequential(
              (0): Linear(in_features=192, out_features=576, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=576, out_features=192, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
  )
  (to_latent): Identity()
  (mlp_head): Sequential(
    (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=192, out_features=10, bias=True)
  )
)
/home/yxma/anaconda3/envs/py39_torch/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/yxma/anaconda3/envs/py39_torch/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
[2025/03/26 02:00:30] - epoch 0 lr 1.000000e-01
start_time 20250326020028
Process ID: 2104463
Files already downloaded and verified
Files already downloaded and verified


No checkpoint. Train from scratch.
Training using fixed weight.


[2025/03/26 02:00:32] - Iter: [0][0/391]	Loss 2.445 (2.445)	Prec@1 0.125 (0.125)	
[2025/03/26 02:01:01] - Iter: [0][50/391]	Loss 2.403 (3.895)	Prec@1 0.031 (0.103)	
[2025/03/26 02:01:31] - Iter: [0][100/391]	Loss 2.351 (3.135)	Prec@1 0.055 (0.102)	
[2025/03/26 02:02:01] - Iter: [0][150/391]	Loss 2.321 (2.872)	Prec@1 0.094 (0.102)	
[2025/03/26 02:02:31] - Iter: [0][200/391]	Loss 2.323 (2.738)	Prec@1 0.109 (0.100)	
[2025/03/26 02:03:00] - Iter: [0][250/391]	Loss 2.305 (2.654)	Prec@1 0.117 (0.100)	
[2025/03/26 02:03:30] - Iter: [0][300/391]	Loss 2.307 (2.597)	Prec@1 0.078 (0.101)	
[2025/03/26 02:04:00] - Iter: [0][350/391]	Loss 2.300 (2.557)	Prec@1 0.133 (0.101)	
[2025/03/26 02:04:24] - Evaluating with standard images with random mask...
tensor([[[[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 1., 0., 1.],
          [0., 0., 0., 0., 0.]]],


        [[[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 1., 0., 1.],
          [0., 0., 0., 0., 0.]]],


        [[[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 1., 0., 1.],
          [0., 0., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 1., 0., 1.],
          [0., 0., 0., 0., 0.]]],


        [[[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 1., 0., 1.],
          [0., 0., 0., 0., 0.]]],


        [[[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 1., 0., 1.],
          [0., 0., 0., 0., 0.]]]], device='cuda:0')
[2025/03/26 02:04:26] - Evaluating with PGD Attack with random mask...
[2025/03/26 02:10:29] - Test Loss: 2.3112  	 Test Acc: 0.1000  
 PGD Loss: 2.3112 	 PGD Acc: 0.1000 
 Best PGD Acc: 0.1000 	 Test Acc of best PGD ckpt: 0.1000
[2025/03/26 02:10:29] - epoch 1 lr 1.000000e-01
New Best Adv Training
[2025/03/26 02:10:30] - Iter: [1][0/391]	Loss 2.320 (2.320)	Prec@1 0.078 (0.078)	
[2025/03/26 02:11:00] - Iter: [1][50/391]	Loss 2.322 (2.309)	Prec@1 0.070 (0.102)	
[2025/03/26 02:11:30] - Iter: [1][100/391]	Loss 2.311 (2.309)	Prec@1 0.086 (0.100)	
[2025/03/26 02:12:00] - Iter: [1][150/391]	Loss 2.300 (2.310)	Prec@1 0.109 (0.101)	
[2025/03/26 02:12:30] - Iter: [1][200/391]	Loss 2.307 (2.310)	Prec@1 0.109 (0.101)	
[2025/03/26 02:12:59] - Iter: [1][250/391]	Loss 2.300 (2.310)	Prec@1 0.055 (0.100)	
[2025/03/26 02:13:29] - Iter: [1][300/391]	Loss 2.316 (2.310)	Prec@1 0.102 (0.100)	
[2025/03/26 02:13:59] - Iter: [1][350/391]	Loss 2.302 (2.309)	Prec@1 0.133 (0.100)	
[2025/03/26 02:14:23] - Evaluating with standard images with random mask...
[2025/03/26 02:14:25] - Evaluating with PGD Attack with random mask...
[2025/03/26 02:20:28] - Test Loss: 2.3041  	 Test Acc: 0.1000  
 PGD Loss: 2.3041 	 PGD Acc: 0.1000 
 Best PGD Acc: 0.1000 	 Test Acc of best PGD ckpt: 0.1000
[2025/03/26 02:20:28] - epoch 2 lr 1.000000e-01
[2025/03/26 02:20:29] - Iter: [2][0/391]	Loss 2.303 (2.303)	Prec@1 0.086 (0.086)	
[2025/03/26 02:20:59] - Iter: [2][50/391]	Loss 2.297 (2.306)	Prec@1 0.125 (0.098)	
[2025/03/26 02:21:29] - Iter: [2][100/391]	Loss 2.315 (2.306)	Prec@1 0.086 (0.099)	
[2025/03/26 02:21:59] - Iter: [2][150/391]	Loss 2.305 (2.307)	Prec@1 0.078 (0.099)	
[2025/03/26 02:22:29] - Iter: [2][200/391]	Loss 2.316 (2.307)	Prec@1 0.047 (0.098)	
[2025/03/26 02:22:58] - Iter: [2][250/391]	Loss 2.305 (2.307)	Prec@1 0.109 (0.098)	
[2025/03/26 02:23:28] - Iter: [2][300/391]	Loss 2.287 (2.307)	Prec@1 0.141 (0.100)	
[2025/03/26 02:23:58] - Iter: [2][350/391]	Loss 2.302 (2.306)	Prec@1 0.086 (0.101)	
[2025/03/26 02:24:22] - Evaluating with standard images with random mask...
[2025/03/26 02:24:25] - Evaluating with PGD Attack with random mask...
[2025/03/26 02:30:28] - Test Loss: 2.3062  	 Test Acc: 0.1000  
 PGD Loss: 2.3062 	 PGD Acc: 0.1000 
 Best PGD Acc: 0.1000 	 Test Acc of best PGD ckpt: 0.1000
[2025/03/26 02:30:28] - epoch 3 lr 1.000000e-01
[2025/03/26 02:30:28] - Iter: [3][0/391]	Loss 2.301 (2.301)	Prec@1 0.117 (0.117)	
[2025/03/26 02:30:58] - Iter: [3][50/391]	Loss 2.299 (2.306)	Prec@1 0.102 (0.100)	
[2025/03/26 02:31:28] - Iter: [3][100/391]	Loss 2.322 (2.305)	Prec@1 0.055 (0.101)	
[2025/03/26 02:31:58] - Iter: [3][150/391]	Loss 2.299 (2.305)	Prec@1 0.086 (0.099)	
[2025/03/26 02:32:28] - Iter: [3][200/391]	Loss 2.308 (2.305)	Prec@1 0.117 (0.100)	
[2025/03/26 02:32:58] - Iter: [3][250/391]	Loss 2.310 (2.305)	Prec@1 0.094 (0.101)	
[2025/03/26 02:33:27] - Iter: [3][300/391]	Loss 2.296 (2.305)	Prec@1 0.133 (0.101)	
[2025/03/26 02:33:57] - Iter: [3][350/391]	Loss 2.302 (2.305)	Prec@1 0.102 (0.100)	
[2025/03/26 02:34:21] - Evaluating with standard images with random mask...
[2025/03/26 02:34:24] - Evaluating with PGD Attack with random mask...
[2025/03/26 02:40:25] - Test Loss: 2.3042  	 Test Acc: 0.1000  
 PGD Loss: 2.3042 	 PGD Acc: 0.1000 
 Best PGD Acc: 0.1000 	 Test Acc of best PGD ckpt: 0.1000
[2025/03/26 02:40:25] - epoch 4 lr 1.000000e-01
[2025/03/26 02:40:26] - Iter: [4][0/391]	Loss 2.308 (2.308)	Prec@1 0.102 (0.102)	
[2025/03/26 02:40:56] - Iter: [4][50/391]	Loss 2.300 (2.304)	Prec@1 0.094 (0.098)	
[2025/03/26 02:41:26] - Iter: [4][100/391]	Loss 2.335 (2.305)	Prec@1 0.094 (0.097)	
[2025/03/26 02:41:55] - Iter: [4][150/391]	Loss 2.307 (2.305)	Prec@1 0.070 (0.100)	
[2025/03/26 02:42:25] - Iter: [4][200/391]	Loss 2.303 (2.305)	Prec@1 0.125 (0.102)	
[2025/03/26 02:42:55] - Iter: [4][250/391]	Loss 2.306 (2.305)	Prec@1 0.070 (0.101)	
[2025/03/26 02:43:24] - Iter: [4][300/391]	Loss 2.306 (2.305)	Prec@1 0.102 (0.101)	
[2025/03/26 02:43:54] - Iter: [4][350/391]	Loss 2.304 (2.305)	Prec@1 0.094 (0.101)	
[2025/03/26 02:44:18] - Evaluating with standard images with random mask...
[2025/03/26 02:44:21] - Evaluating with PGD Attack with random mask...
[2025/03/26 02:50:22] - Test Loss: 2.3041  	 Test Acc: 0.1000  
 PGD Loss: 2.3041 	 PGD Acc: 0.1000 
 Best PGD Acc: 0.1000 	 Test Acc of best PGD ckpt: 0.1000
[2025/03/26 02:50:22] - epoch 5 lr 1.000000e-01
[2025/03/26 02:50:23] - Iter: [5][0/391]	Loss 2.303 (2.303)	Prec@1 0.117 (0.117)	
[2025/03/26 02:50:53] - Iter: [5][50/391]	Loss 2.300 (2.304)	Prec@1 0.133 (0.102)	
[2025/03/26 02:51:22] - Iter: [5][100/391]	Loss 2.296 (2.304)	Prec@1 0.109 (0.100)	
[2025/03/26 02:51:52] - Iter: [5][150/391]	Loss 2.295 (2.304)	Prec@1 0.172 (0.099)	
[2025/03/26 02:52:22] - Iter: [5][200/391]	Loss 2.304 (2.304)	Prec@1 0.094 (0.099)	
[2025/03/26 02:52:51] - Iter: [5][250/391]	Loss 2.305 (2.304)	Prec@1 0.078 (0.099)	
[2025/03/26 02:53:21] - Iter: [5][300/391]	Loss 2.291 (2.304)	Prec@1 0.172 (0.100)	
[2025/03/26 02:53:51] - Iter: [5][350/391]	Loss 2.304 (2.304)	Prec@1 0.078 (0.100)	
[2025/03/26 02:54:14] - Evaluating with standard images with random mask...
[2025/03/26 02:54:17] - Evaluating with PGD Attack with random mask...
[2025/03/26 03:00:19] - Test Loss: 2.3044  	 Test Acc: 0.1000  
 PGD Loss: 2.3044 	 PGD Acc: 0.1000 
 Best PGD Acc: 0.1000 	 Test Acc of best PGD ckpt: 0.1000
[2025/03/26 03:00:19] - epoch 6 lr 1.000000e-01
[2025/03/26 03:00:19] - Iter: [6][0/391]	Loss 2.308 (2.308)	Prec@1 0.047 (0.047)	
[2025/03/26 03:00:49] - Iter: [6][50/391]	Loss 2.307 (2.305)	Prec@1 0.102 (0.096)	
[2025/03/26 03:01:19] - Iter: [6][100/391]	Loss 2.310 (2.304)	Prec@1 0.109 (0.100)	
[2025/03/26 03:01:49] - Iter: [6][150/391]	Loss 2.299 (2.304)	Prec@1 0.141 (0.100)	
[2025/03/26 03:02:18] - Iter: [6][200/391]	Loss 2.297 (2.305)	Prec@1 0.133 (0.099)	
[2025/03/26 03:02:48] - Iter: [6][250/391]	Loss 2.299 (2.304)	Prec@1 0.141 (0.100)	
[2025/03/26 03:03:18] - Iter: [6][300/391]	Loss 2.306 (2.304)	Prec@1 0.078 (0.100)	
[2025/03/26 03:03:47] - Iter: [6][350/391]	Loss 2.301 (2.304)	Prec@1 0.117 (0.100)	
[2025/03/26 03:04:11] - Evaluating with standard images with random mask...
[2025/03/26 03:04:14] - Evaluating with PGD Attack with random mask...
[2025/03/26 03:10:16] - Test Loss: 2.3064  	 Test Acc: 0.1000  
 PGD Loss: 2.3064 	 PGD Acc: 0.1000 
 Best PGD Acc: 0.1000 	 Test Acc of best PGD ckpt: 0.1000
[2025/03/26 03:10:16] - epoch 7 lr 1.000000e-01
[2025/03/26 03:10:17] - Iter: [7][0/391]	Loss 2.303 (2.303)	Prec@1 0.055 (0.055)	
[2025/03/26 03:10:46] - Iter: [7][50/391]	Loss 2.303 (2.305)	Prec@1 0.094 (0.099)	
[2025/03/26 03:11:16] - Iter: [7][100/391]	Loss 2.312 (2.305)	Prec@1 0.102 (0.100)	
[2025/03/26 03:11:46] - Iter: [7][150/391]	Loss 2.299 (2.305)	Prec@1 0.125 (0.100)	
[2025/03/26 03:12:16] - Iter: [7][200/391]	Loss 2.306 (2.304)	Prec@1 0.078 (0.100)	
[2025/03/26 03:12:46] - Iter: [7][250/391]	Loss 2.307 (2.304)	Prec@1 0.102 (0.100)	
[2025/03/26 03:13:16] - Iter: [7][300/391]	Loss 2.300 (2.304)	Prec@1 0.062 (0.100)	
[2025/03/26 03:13:45] - Iter: [7][350/391]	Loss 2.302 (2.304)	Prec@1 0.109 (0.100)	
[2025/03/26 03:14:09] - Evaluating with standard images with random mask...
[2025/03/26 03:14:12] - Evaluating with PGD Attack with random mask...
[2025/03/26 03:20:15] - Test Loss: 2.3040  	 Test Acc: 0.1000  
 PGD Loss: 2.3040 	 PGD Acc: 0.1000 
 Best PGD Acc: 0.1000 	 Test Acc of best PGD ckpt: 0.1000
[2025/03/26 03:20:15] - epoch 8 lr 1.000000e-01
[2025/03/26 03:20:16] - Iter: [8][0/391]	Loss 2.299 (2.299)	Prec@1 0.109 (0.109)	
[2025/03/26 03:20:46] - Iter: [8][50/391]	Loss 2.313 (2.305)	Prec@1 0.062 (0.105)	
[2025/03/26 03:21:15] - Iter: [8][100/391]	Loss 2.314 (2.304)	Prec@1 0.086 (0.103)	
[2025/03/26 03:21:45] - Iter: [8][150/391]	Loss 2.309 (2.304)	Prec@1 0.086 (0.100)	
[2025/03/26 03:22:15] - Iter: [8][200/391]	Loss 2.296 (2.304)	Prec@1 0.094 (0.100)	
[2025/03/26 03:22:45] - Iter: [8][250/391]	Loss 2.295 (2.305)	Prec@1 0.148 (0.099)	
[2025/03/26 03:23:15] - Iter: [8][300/391]	Loss 2.301 (2.305)	Prec@1 0.141 (0.100)	
[2025/03/26 03:23:44] - Iter: [8][350/391]	Loss 2.302 (2.305)	Prec@1 0.117 (0.100)	
