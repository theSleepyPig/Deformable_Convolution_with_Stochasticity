nohup: ignoring input
wandb: Currently logged in as: xuanzhu_07 (xuanzhu_07-university-of-sydney). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/yxma/hzx/hzx/rand_defence/wandb/run-20241119_160101-069kultz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-plant-11
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xuanzhu_07-university-of-sydney/findboundary
wandb: üöÄ View run at https://wandb.ai/xuanzhu_07-university-of-sydney/findboundary/runs/069kultz
[2024/11/19 16:01:04] - Namespace(batch_size=128, data_dir='~/datasets/CIFAR/', dataset='cifar10', epochs=200, network='ResNet18', worker=4, lr_schedule='multistep', lr_min=0.0, lr_max=0.1, weight_decay=0.0005, momentum=0.9, none_random_training=True, rand_deform_training=False, randpos_deform_training=True, randpos_multi_deform_training=False, is_n_repeat=True, reNum=53, only_adv_randpos_training=False, rand_path_training=False, epsilon=8, alpha=2, seed=0, attack_iters=7, restarts=1, none_adv_training=False, save_dir='ckpt/cifar10/ResNet18/ckpt', pretrain=None, continue_training=False, lb=2048, pos=0, eot=False, hang=False, device=0, num_classes=10)
[2024/11/19 16:01:06] - ResNetPartmask5(
  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layer1): ModuleList(
    (0): RandonBasicBlock203(
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): MaskedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential()
    )
  )
  (layer11): ModuleList(
    (0): BasicBlock(
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential()
    )
  )
  (layer2): ModuleList(
    (0): BasicBlock(
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential()
    )
  )
  (layer3): ModuleList(
    (0): BasicBlock(
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential()
    )
  )
  (layer4): ModuleList(
    (0): BasicBlock(
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (shortcut): Sequential()
    )
  )
  (linear): Linear(in_features=512, out_features=10, bias=True)
)
/home/yxma/anaconda3/envs/py39_torch/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/yxma/anaconda3/envs/py39_torch/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
[2024/11/19 16:01:06] - epoch 0 lr 1.000000e-01
start_time 20241119160104
Process ID: 71368
Files already downloaded and verified
Files already downloaded and verified


No checkpoint. Train from scratch.
Training using fixed weight.


tensor([[[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]],


        [[[1., 1., 1.],
          [0., 0., 1.],
          [0., 1., 0.]]]], device='cuda:0')
torch.Size([64, 1, 3, 3])
Traceback (most recent call last):
  File "/home/yxma/hzx/hzx/rand_defence/train.py", line 445, in <module>
    main()
  File "/home/yxma/hzx/hzx/rand_defence/train.py", line 311, in main
    modelto.set_rand_mask()
  File "/home/yxma/hzx/hzx/rand_defence/model/randpos_multi_resnet.py", line 3341, in set_rand_mask
    layer.set_rand_mask1()
  File "/home/yxma/hzx/hzx/rand_defence/model/randpos_multi_resnet.py", line 991, in set_rand_mask1
    selected_top = np.random.choice(available_choices, 5, replace=False)
  File "numpy/random/mtrand.pyx", line 1001, in numpy.random.mtrand.RandomState.choice
ValueError: Cannot take a larger sample than population when 'replace=False'
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.005 MB uploadedwandb: / 0.005 MB of 0.005 MB uploadedwandb: - 0.009 MB of 0.016 MB uploadedwandb: \ 0.012 MB of 0.027 MB uploadedwandb: | 0.023 MB of 0.027 MB uploadedwandb: / 0.023 MB of 0.027 MB uploadedwandb: - 0.027 MB of 0.027 MB uploadedwandb: üöÄ View run wandering-plant-11 at: https://wandb.ai/xuanzhu_07-university-of-sydney/findboundary/runs/069kultz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/xuanzhu_07-university-of-sydney/findboundary
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241119_160101-069kultz/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
