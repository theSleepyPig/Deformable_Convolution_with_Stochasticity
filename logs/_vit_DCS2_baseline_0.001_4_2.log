nohup: ignoring input
wandb: Currently logged in as: xuanzhu_07 (xuanzhu_07-university-of-sydney). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.19.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/yxma/hzx/hzx/rand_defence/wandb/run-20250328_004227-uqobharp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-microwave-207
wandb: â­ï¸ View project at https://wandb.ai/xuanzhu_07-university-of-sydney/findVitVit
wandb: ðŸš€ View run at https://wandb.ai/xuanzhu_07-university-of-sydney/findVitVit/runs/uqobharp
[2025/03/28 00:42:32] - Namespace(batch_size=128, data_dir='~/datasets/CIFAR/', dataset='cifar10', epochs=200, network='earlyVit', worker=4, lr_schedule='multistep', lr_min=0.0, lr_max=0.001, weight_decay=0.0005, momentum=0.9, none_random_training=True, rand_deform_training=False, randpos_deform_training=False, randpos_multi_deform_training=False, only_adv_randpos_training=False, rand_path_training=False, layerNum=2, randType='repeated', epsilon=8, alpha=2, seed=0, attack_iters=7, restarts=1, none_adv_training=False, save_dir='ckpt/cifar10/earlyVit/ckpt', pretrain=None, continue_training=False, lb=2048, pos=0, eot=False, hang=False, device=1, optimizer='sgd', num_classes=10)
[2025/03/28 00:42:34] - EarlyConvViT(
  (conv_layers): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (1): Sequential(
      (0): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (2): Sequential(
      (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (3): Sequential(
      (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (conv_1x1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
    (flatten image): Rearrange('batch channels height width -> batch (height width) channels')
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (transformer): Transformer(
    (layers): ModuleList(
      (0-10): 11 x ModuleList(
        (0): PreNorm(
          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (fn): Attention(
            (attend): Softmax(dim=-1)
            (to_qkv): Linear(in_features=192, out_features=576, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=192, out_features=192, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): PreNorm(
          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (fn): FeedForward(
            (net): Sequential(
              (0): Linear(in_features=192, out_features=576, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=576, out_features=192, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
  )
  (to_latent): Identity()
  (mlp_head): Sequential(
    (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=192, out_features=10, bias=True)
  )
)
/home/yxma/anaconda3/envs/py39_torch/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/yxma/anaconda3/envs/py39_torch/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
[2025/03/28 00:42:34] - epoch 0 lr 1.000000e-03
start_time 20250328004232
Process ID: 1121370
Files already downloaded and verified
Files already downloaded and verified
303030


No checkpoint. Train from scratch.
Training using fixed weight.


[2025/03/28 00:42:36] - Iter: [0][0/391]	Loss 2.498 (2.498)	Prec@1 0.117 (0.117)	
[2025/03/28 00:43:05] - Iter: [0][50/391]	Loss 2.358 (2.383)	Prec@1 0.023 (0.052)	
[2025/03/28 00:43:34] - Iter: [0][100/391]	Loss 2.323 (2.365)	Prec@1 0.062 (0.046)	
[2025/03/28 00:44:03] - Iter: [0][150/391]	Loss 2.343 (2.353)	Prec@1 0.086 (0.057)	
[2025/03/28 00:44:33] - Iter: [0][200/391]	Loss 2.316 (2.344)	Prec@1 0.094 (0.064)	
[2025/03/28 00:45:02] - Iter: [0][250/391]	Loss 2.313 (2.336)	Prec@1 0.109 (0.072)	
[2025/03/28 00:45:31] - Iter: [0][300/391]	Loss 2.289 (2.328)	Prec@1 0.078 (0.082)	
[2025/03/28 00:46:00] - Iter: [0][350/391]	Loss 2.290 (2.321)	Prec@1 0.125 (0.089)	
[2025/03/28 00:46:23] - Evaluating with standard images...
[2025/03/28 00:46:26] - Evaluating with PGD Attack...
[2025/03/28 00:48:24] - Test Loss: 2.1868  	 Test Acc: 0.1796  
 PGD Loss: 2.2609 	 PGD Acc: 0.1470 
 Best PGD Acc: 0.1470 	 Test Acc of best PGD ckpt: 0.1796
[2025/03/28 00:48:24] - epoch 1 lr 1.000000e-03
New Best Adv Training
[2025/03/28 00:48:25] - Iter: [1][0/391]	Loss 2.244 (2.244)	Prec@1 0.141 (0.141)	
[2025/03/28 00:48:54] - Iter: [1][50/391]	Loss 2.273 (2.261)	Prec@1 0.141 (0.135)	
[2025/03/28 00:49:23] - Iter: [1][100/391]	Loss 2.347 (2.264)	Prec@1 0.117 (0.139)	
[2025/03/28 00:49:52] - Iter: [1][150/391]	Loss 2.244 (2.261)	Prec@1 0.164 (0.140)	
[2025/03/28 00:50:21] - Iter: [1][200/391]	Loss 2.294 (2.259)	Prec@1 0.117 (0.142)	
[2025/03/28 00:50:50] - Iter: [1][250/391]	Loss 2.263 (2.258)	Prec@1 0.148 (0.140)	
[2025/03/28 00:51:19] - Iter: [1][300/391]	Loss 2.241 (2.257)	Prec@1 0.148 (0.142)	
[2025/03/28 00:51:48] - Iter: [1][350/391]	Loss 2.235 (2.255)	Prec@1 0.203 (0.144)	
[2025/03/28 00:52:11] - Evaluating with standard images...
[2025/03/28 00:52:14] - Evaluating with PGD Attack...
[2025/03/28 00:54:12] - Test Loss: 2.1632  	 Test Acc: 0.2014  
 PGD Loss: 2.2357 	 PGD Acc: 0.1675 
 Best PGD Acc: 0.1675 	 Test Acc of best PGD ckpt: 0.2014
[2025/03/28 00:54:12] - epoch 2 lr 1.000000e-03
New Best Adv Training
[2025/03/28 00:54:13] - Iter: [2][0/391]	Loss 2.262 (2.262)	Prec@1 0.148 (0.148)	
[2025/03/28 00:54:42] - Iter: [2][50/391]	Loss 2.197 (2.236)	Prec@1 0.195 (0.152)	
[2025/03/28 00:55:11] - Iter: [2][100/391]	Loss 2.228 (2.235)	Prec@1 0.180 (0.156)	
[2025/03/28 00:55:40] - Iter: [2][150/391]	Loss 2.236 (2.234)	Prec@1 0.172 (0.157)	
[2025/03/28 00:56:09] - Iter: [2][200/391]	Loss 2.206 (2.233)	Prec@1 0.195 (0.161)	
[2025/03/28 00:56:38] - Iter: [2][250/391]	Loss 2.252 (2.231)	Prec@1 0.156 (0.161)	
[2025/03/28 00:57:08] - Iter: [2][300/391]	Loss 2.234 (2.228)	Prec@1 0.117 (0.161)	
[2025/03/28 00:57:37] - Iter: [2][350/391]	Loss 2.256 (2.227)	Prec@1 0.172 (0.162)	
[2025/03/28 00:58:00] - Evaluating with standard images...
[2025/03/28 00:58:03] - Evaluating with PGD Attack...
[2025/03/28 01:00:01] - Test Loss: 2.1233  	 Test Acc: 0.2181  
 PGD Loss: 2.2061 	 PGD Acc: 0.1773 
 Best PGD Acc: 0.1773 	 Test Acc of best PGD ckpt: 0.2181
[2025/03/28 01:00:01] - epoch 3 lr 1.000000e-03
New Best Adv Training
[2025/03/28 01:00:01] - Iter: [3][0/391]	Loss 2.230 (2.230)	Prec@1 0.203 (0.203)	
[2025/03/28 01:00:31] - Iter: [3][50/391]	Loss 2.222 (2.221)	Prec@1 0.172 (0.166)	
[2025/03/28 01:01:00] - Iter: [3][100/391]	Loss 2.228 (2.218)	Prec@1 0.172 (0.171)	
[2025/03/28 01:01:29] - Iter: [3][150/391]	Loss 2.148 (2.209)	Prec@1 0.195 (0.171)	
[2025/03/28 01:01:58] - Iter: [3][200/391]	Loss 2.249 (2.207)	Prec@1 0.180 (0.169)	
[2025/03/28 01:02:27] - Iter: [3][250/391]	Loss 2.168 (2.205)	Prec@1 0.211 (0.171)	
[2025/03/28 01:02:56] - Iter: [3][300/391]	Loss 2.239 (2.206)	Prec@1 0.117 (0.171)	
[2025/03/28 01:03:25] - Iter: [3][350/391]	Loss 2.162 (2.206)	Prec@1 0.180 (0.171)	
[2025/03/28 01:03:48] - Evaluating with standard images...
[2025/03/28 01:03:51] - Evaluating with PGD Attack...
[2025/03/28 01:05:49] - Test Loss: 2.0969  	 Test Acc: 0.2254  
 PGD Loss: 2.1860 	 PGD Acc: 0.1822 
 Best PGD Acc: 0.1822 	 Test Acc of best PGD ckpt: 0.2254
[2025/03/28 01:05:49] - epoch 4 lr 1.000000e-03
New Best Adv Training
[2025/03/28 01:05:50] - Iter: [4][0/391]	Loss 2.240 (2.240)	Prec@1 0.148 (0.148)	
[2025/03/28 01:06:19] - Iter: [4][50/391]	Loss 2.241 (2.196)	Prec@1 0.164 (0.175)	
[2025/03/28 01:06:48] - Iter: [4][100/391]	Loss 2.189 (2.195)	Prec@1 0.180 (0.176)	
[2025/03/28 01:07:17] - Iter: [4][150/391]	Loss 2.171 (2.197)	Prec@1 0.164 (0.176)	
[2025/03/28 01:07:46] - Iter: [4][200/391]	Loss 2.205 (2.195)	Prec@1 0.141 (0.175)	
[2025/03/28 01:08:15] - Iter: [4][250/391]	Loss 2.206 (2.195)	Prec@1 0.195 (0.176)	
[2025/03/28 01:08:44] - Iter: [4][300/391]	Loss 2.191 (2.194)	Prec@1 0.133 (0.177)	
[2025/03/28 01:09:14] - Iter: [4][350/391]	Loss 2.193 (2.196)	Prec@1 0.125 (0.177)	
[2025/03/28 01:09:37] - Evaluating with standard images...
[2025/03/28 01:09:39] - Evaluating with PGD Attack...
[2025/03/28 01:11:37] - Test Loss: 2.0875  	 Test Acc: 0.2361  
 PGD Loss: 2.1797 	 PGD Acc: 0.1770 
 Best PGD Acc: 0.1822 	 Test Acc of best PGD ckpt: 0.2254
[2025/03/28 01:11:37] - epoch 5 lr 1.000000e-03
[2025/03/28 01:11:38] - Iter: [5][0/391]	Loss 2.155 (2.155)	Prec@1 0.180 (0.180)	
[2025/03/28 01:12:07] - Iter: [5][50/391]	Loss 2.168 (2.188)	Prec@1 0.219 (0.181)	
[2025/03/28 01:12:37] - Iter: [5][100/391]	Loss 2.154 (2.193)	Prec@1 0.203 (0.177)	
[2025/03/28 01:13:06] - Iter: [5][150/391]	Loss 2.226 (2.193)	Prec@1 0.195 (0.179)	
[2025/03/28 01:13:35] - Iter: [5][200/391]	Loss 2.164 (2.192)	Prec@1 0.156 (0.179)	
[2025/03/28 01:14:04] - Iter: [5][250/391]	Loss 2.140 (2.193)	Prec@1 0.164 (0.179)	
[2025/03/28 01:14:33] - Iter: [5][300/391]	Loss 2.161 (2.192)	Prec@1 0.164 (0.179)	
