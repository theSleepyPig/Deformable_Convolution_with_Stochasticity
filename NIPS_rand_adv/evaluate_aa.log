nohup: ignoring input
wandb: Currently logged in as: xuanzhu_07 (xuanzhu_07-university-of-sydney). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/yxma/hzx/NIPS_rand_adv/wandb/run-20250131_064102-poqonyys
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-tree-81
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xuanzhu_07-university-of-sydney/hole_imagenet
wandb: üöÄ View run at https://wandb.ai/xuanzhu_07-university-of-sydney/hole_imagenet/runs/poqonyys
[2025/01/31 06:41:05] - ***************************** LOGISTICS *****************************
***************************** LOGISTICS *****************************
[2025/01/31 06:41:05] - Experiment Date: 2025-01-31 06:41
Experiment Date: 2025-01-31 06:41
[2025/01/31 06:41:05] - Output Name: ckpt
Output Name: ckpt
[2025/01/31 06:41:05] - User: yxma
User: yxma
[2025/01/31 06:41:05] - ***************************** ARGUMENTS *****************************
***************************** ARGUMENTS *****************************
[2025/01/31 06:41:05] - TRAIN: {'epochs': 90, 'arch': 'resnet50', 'start_epoch': 0, 'step': 0, 'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001, 'print_freq': 20, 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225)}
TRAIN: {'epochs': 90, 'arch': 'resnet50', 'start_epoch': 0, 'step': 0, 'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001, 'print_freq': 20, 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225)}
[2025/01/31 06:41:05] - ADV: {'clip_eps': 0.01568627450980392, 'fgsm_step': 0.01568627450980392, 'n_repeats': 1, 'pgd_attack': [(10, 0.00392156862), (50, 0.00392156862)], 'delta_init': 'zero', 'attack_iters': 2}
ADV: {'clip_eps': 0.01568627450980392, 'fgsm_step': 0.01568627450980392, 'n_repeats': 1, 'pgd_attack': [(10, 0.00392156862), (50, 0.00392156862)], 'delta_init': 'zero', 'attack_iters': 2}
[2025/01/31 06:41:05] - DATA: {'workers': 4, 'batch_size': 512, 'img_size': 256, 'crop_size': 224, 'max_color_value': 255.0}
DATA: {'workers': 4, 'batch_size': 512, 'img_size': 256, 'crop_size': 224, 'max_color_value': 255.0}
[2025/01/31 06:41:05] - data: /mnt/ssd_1/ykwang/imagenet
data: /mnt/ssd_1/ykwang/imagenet
[2025/01/31 06:41:05] - config: config.yml
config: config.yml
[2025/01/31 06:41:05] - evaluate: True
evaluate: True
[2025/01/31 06:41:05] - eval_model_path: /home/yxma/hzx/NIPS_rand_adv/ckpt/model_best_20240828_140015.pth
eval_model_path: /home/yxma/hzx/NIPS_rand_adv/ckpt/model_best_20240828_140015.pth
[2025/01/31 06:41:05] - pretrained: True
pretrained: True
[2025/01/31 06:41:05] - lr: 0.1
lr: 0.1
[2025/01/31 06:41:05] - lr_schedule: cosine
lr_schedule: cosine
[2025/01/31 06:41:05] - epochs: 90
epochs: 90
[2025/01/31 06:41:05] - batch_size: 512
batch_size: 512
[2025/01/31 06:41:05] - clip_eps: 4
clip_eps: 4
[2025/01/31 06:41:05] - fgsm_step: 4
fgsm_step: 4
[2025/01/31 06:41:05] - save_dir: ckpt
save_dir: ckpt
[2025/01/31 06:41:05] - adv_train: True
adv_train: True
[2025/01/31 06:41:05] - rd: True
rd: True
[2025/01/31 06:41:05] - rp_out_channel: 0
rp_out_channel: 0
[2025/01/31 06:41:05] - rp_weight_decay: 0.0005
rp_weight_decay: 0.0005
[2025/01/31 06:41:05] - wandb_project: hole_imagenet
wandb_project: hole_imagenet
[2025/01/31 06:41:05] - wandb_name: xuanzhu_07-university-of-sydney
wandb_name: xuanzhu_07-university-of-sydney
[2025/01/31 06:41:05] - output_name: ckpt
output_name: ckpt
[2025/01/31 06:41:05] - num: 1000
num: 1000
[2025/01/31 06:41:05] - **********************************************************************
**********************************************************************
[2025/01/31 06:41:06] - DataParallel(
  (module): ResNetmask(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck2(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): MaskedConv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (layer12): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
)
DataParallel(
  (module): ResNetmask(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck2(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): MaskedConv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (layer12): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
)
[2025/01/31 06:41:06] - Train from scratch
Train from scratch
[2025/01/31 06:41:08] - => Automatic resume from '/home/yxma/hzx/NIPS_rand_adv/ckpt/model_best_20240828_140015.pth' (epoch 83)
=> Automatic resume from '/home/yxma/hzx/NIPS_rand_adv/ckpt/model_best_20240828_140015.pth' (epoch 83)
[2025/01/31 06:41:08] - *********************** Performing PGD Attacks ***********************
*********************** Performing PGD Attacks ***********************
Training started at: 20250131_064105
Process ID: 2803820
True
-------------------Loading Pretrained Model--------------------
Skipping loading parameter layer1.0.conv2.weight due to shape mismatch: expected torch.Size([64, 64, 5, 5]), got torch.Size([64, 64, 3, 3])
Successfully matched and loaded 266 layers.
[2025/01/31 06:41:19] - Test: [0/98]	Time 10.754 (10.754)	Loss 0.8987 (0.8987)	Prec@1 77.148 (77.148)	Prec@5 93.164 (93.164)
Test: [0/98]	Time 10.754 (10.754)	Loss 0.8987 (0.8987)	Prec@1 77.148 (77.148)	Prec@5 93.164 (93.164)
tensor([[[[0., 0., 0., 0., 0.],
          [0., 0., 0., 1., 0.],
          [0., 0., 1., 0., 0.],
          [0., 0., 1., 0., 0.],
          [1., 0., 0., 0., 0.]]],


        [[[0., 0., 0., 0., 0.],
          [0., 0., 0., 1., 0.],
          [0., 0., 1., 0., 0.],
          [0., 0., 1., 0., 0.],
          [1., 0., 0., 0., 0.]]],


        [[[0., 0., 0., 0., 0.],
          [0., 0., 0., 1., 0.],
          [0., 0., 1., 0., 0.],
          [0., 0., 1., 0., 0.],
          [1., 0., 0., 0., 0.]]],


        ...,


        [[[0., 0., 0., 0., 0.],
          [0., 0., 0., 1., 0.],
          [0., 0., 1., 0., 0.],
          [0., 0., 1., 0., 0.],
          [1., 0., 0., 0., 0.]]],


        [[[0., 0., 0., 0., 0.],
          [0., 0., 0., 1., 0.],
          [0., 0., 1., 0., 0.],
          [0., 0., 1., 0., 0.],
          [1., 0., 0., 0., 0.]]],


        [[[0., 0., 0., 0., 0.],
          [0., 0., 0., 1., 0.],
          [0., 0., 1., 0., 0.],
          [0., 0., 1., 0., 0.],
          [1., 0., 0., 0., 0.]]]], device='cuda:0')
[2025/01/31 06:41:28] - Test: [20/98]	Time 0.110 (0.932)	Loss 1.2364 (1.0252)	Prec@1 66.211 (74.247)	Prec@5 93.359 (91.676)
Test: [20/98]	Time 0.110 (0.932)	Loss 1.2364 (1.0252)	Prec@1 66.211 (74.247)	Prec@5 93.359 (91.676)
[2025/01/31 06:41:42] - Test: [40/98]	Time 0.110 (0.812)	Loss 1.8425 (1.0349)	Prec@1 58.398 (73.876)	Prec@5 82.617 (91.930)
Test: [40/98]	Time 0.110 (0.812)	Loss 1.8425 (1.0349)	Prec@1 58.398 (73.876)	Prec@5 82.617 (91.930)
[2025/01/31 06:41:55] - Test: [60/98]	Time 0.109 (0.773)	Loss 1.9738 (1.1917)	Prec@1 53.711 (70.876)	Prec@5 79.492 (89.757)
Test: [60/98]	Time 0.109 (0.773)	Loss 1.9738 (1.1917)	Prec@1 53.711 (70.876)	Prec@5 79.492 (89.757)
[2025/01/31 06:42:08] - Test: [80/98]	Time 0.109 (0.743)	Loss 1.6837 (1.2908)	Prec@1 60.547 (69.015)	Prec@5 84.961 (88.298)
Test: [80/98]	Time 0.109 (0.743)	Loss 1.6837 (1.2908)	Prec@1 60.547 (69.015)	Prec@5 84.961 (88.298)
[2025/01/31 06:42:21] -  Final Prec@1 68.140 Prec@5 87.734
 Final Prec@1 68.140 Prec@5 87.734
[2025/01/31 06:42:21] - * AutoAttack eps: 0.03137254901960784, norm: Linf, version: standard *
* AutoAttack eps: 0.03137254901960784, norm: Linf, version: standard *
AA Mask attacking
[2025/01/31 06:53:18] - AA Test: [0/98]	Time 0.000 (328.342)	Loss 0.8648 (0.8648)	Prec@1 79.102 (79.102)	Prec@5 94.922 (94.922)
AA Test: [0/98]	Time 0.000 (328.342)	Loss 0.8648 (0.8648)	Prec@1 79.102 (79.102)	Prec@5 94.922 (94.922)
[2025/01/31 06:53:20] - AA Test: [1/98]	Time 0.000 (164.740)	Loss 0.4353 (0.6500)	Prec@1 90.234 (84.668)	Prec@5 95.703 (95.312)
AA Test: [1/98]	Time 0.000 (164.740)	Loss 0.4353 (0.6500)	Prec@1 90.234 (84.668)	Prec@5 95.703 (95.312)
[2025/01/31 06:53:32] - AA Test: [2/98]	Time 0.000 (111.780)	Loss 0.7436 (0.6812)	Prec@1 84.180 (84.505)	Prec@5 94.922 (95.182)
AA Test: [2/98]	Time 0.000 (111.780)	Loss 0.7436 (0.6812)	Prec@1 84.180 (84.505)	Prec@5 94.922 (95.182)
[2025/01/31 06:53:34] - AA Test: [3/98]	Time 0.000 (84.142)	Loss 1.2918 (0.8339)	Prec@1 63.672 (79.297)	Prec@5 90.820 (94.092)
AA Test: [3/98]	Time 0.000 (84.142)	Loss 1.2918 (0.8339)	Prec@1 63.672 (79.297)	Prec@5 90.820 (94.092)
[2025/01/31 06:53:34] - AA Test: [4/98]	Time 0.000 (67.345)	Loss 1.5978 (0.9866)	Prec@1 62.695 (75.977)	Prec@5 83.984 (92.070)
AA Test: [4/98]	Time 0.000 (67.345)	Loss 1.5978 (0.9866)	Prec@1 62.695 (75.977)	Prec@5 83.984 (92.070)
[2025/01/31 06:53:36] - AA Test: [5/98]	Time 0.000 (56.233)	Loss 1.5124 (1.0743)	Prec@1 62.891 (73.796)	Prec@5 87.305 (91.276)
AA Test: [5/98]	Time 0.000 (56.233)	Loss 1.5124 (1.0743)	Prec@1 62.891 (73.796)	Prec@5 87.305 (91.276)
[2025/01/31 07:23:28] - AA Test: [6/98]	Time 0.000 (176.245)	Loss 1.5919 (1.1482)	Prec@1 57.422 (71.456)	Prec@5 81.250 (89.844)
AA Test: [6/98]	Time 0.000 (176.245)	Loss 1.5919 (1.1482)	Prec@1 57.422 (71.456)	Prec@5 81.250 (89.844)
[2025/01/31 07:50:41] - AA Test: [7/98]	Time 0.000 (256.232)	Loss 0.8481 (1.1107)	Prec@1 77.148 (72.168)	Prec@5 94.727 (90.454)
AA Test: [7/98]	Time 0.000 (256.232)	Loss 0.8481 (1.1107)	Prec@1 77.148 (72.168)	Prec@5 94.727 (90.454)
[2025/01/31 07:50:44] - AA Test: [8/98]	Time 0.000 (227.927)	Loss 0.5011 (1.0430)	Prec@1 89.062 (74.045)	Prec@5 96.289 (91.102)
AA Test: [8/98]	Time 0.000 (227.927)	Loss 0.5011 (1.0430)	Prec@1 89.062 (74.045)	Prec@5 96.289 (91.102)
[2025/01/31 07:50:46] - AA Test: [9/98]	Time 0.000 (205.260)	Loss 0.5533 (0.9940)	Prec@1 86.133 (75.254)	Prec@5 95.312 (91.523)
AA Test: [9/98]	Time 0.000 (205.260)	Loss 0.5533 (0.9940)	Prec@1 86.133 (75.254)	Prec@5 95.312 (91.523)
[2025/01/31 08:17:08] - AA Test: [10/98]	Time 0.000 (258.512)	Loss 1.3121 (1.0229)	Prec@1 70.703 (74.840)	Prec@5 85.352 (90.962)
AA Test: [10/98]	Time 0.000 (258.512)	Loss 1.3121 (1.0229)	Prec@1 70.703 (74.840)	Prec@5 85.352 (90.962)
[2025/01/31 08:17:09] - AA Test: [11/98]	Time 0.000 (237.001)	Loss 1.2989 (1.0459)	Prec@1 71.289 (74.544)	Prec@5 88.672 (90.771)
AA Test: [11/98]	Time 0.000 (237.001)	Loss 1.2989 (1.0459)	Prec@1 71.289 (74.544)	Prec@5 88.672 (90.771)
[2025/01/31 08:28:08] - AA Test: [12/98]	Time 0.000 (244.123)	Loss 0.9648 (1.0397)	Prec@1 79.492 (74.925)	Prec@5 91.602 (90.835)
AA Test: [12/98]	Time 0.000 (244.123)	Loss 0.9648 (1.0397)	Prec@1 79.492 (74.925)	Prec@5 91.602 (90.835)
[2025/01/31 08:28:10] - AA Test: [13/98]	Time 0.000 (226.754)	Loss 0.4306 (0.9962)	Prec@1 89.453 (75.963)	Prec@5 96.289 (91.225)
AA Test: [13/98]	Time 0.000 (226.754)	Loss 0.4306 (0.9962)	Prec@1 89.453 (75.963)	Prec@5 96.289 (91.225)
[2025/01/31 08:28:12] - AA Test: [14/98]	Time 0.000 (211.711)	Loss 0.7623 (0.9806)	Prec@1 81.250 (76.315)	Prec@5 94.922 (91.471)
AA Test: [14/98]	Time 0.000 (211.711)	Loss 0.7623 (0.9806)	Prec@1 81.250 (76.315)	Prec@5 94.922 (91.471)
[2025/01/31 08:28:13] - AA Test: [15/98]	Time 0.000 (198.503)	Loss 1.1007 (0.9881)	Prec@1 72.461 (76.074)	Prec@5 90.820 (91.431)
AA Test: [15/98]	Time 0.000 (198.503)	Loss 1.1007 (0.9881)	Prec@1 72.461 (76.074)	Prec@5 90.820 (91.431)
[2025/01/31 08:28:14] - AA Test: [16/98]	Time 0.000 (186.842)	Loss 1.4305 (1.0141)	Prec@1 62.500 (75.276)	Prec@5 88.477 (91.257)
AA Test: [16/98]	Time 0.000 (186.842)	Loss 1.4305 (1.0141)	Prec@1 62.500 (75.276)	Prec@5 88.477 (91.257)
[2025/01/31 08:28:15] - AA Test: [17/98]	Time 0.000 (176.508)	Loss 1.0851 (1.0181)	Prec@1 72.070 (75.098)	Prec@5 92.188 (91.309)
AA Test: [17/98]	Time 0.000 (176.508)	Loss 1.0851 (1.0181)	Prec@1 72.070 (75.098)	Prec@5 92.188 (91.309)
[2025/01/31 09:01:04] - AA Test: [18/98]	Time 0.000 (219.029)	Loss 1.3214 (1.0340)	Prec@1 67.969 (74.722)	Prec@5 89.062 (91.190)
AA Test: [18/98]	Time 0.000 (219.029)	Loss 1.3214 (1.0340)	Prec@1 67.969 (74.722)	Prec@5 89.062 (91.190)
[2025/01/31 09:01:05] - AA Test: [19/98]	Time 0.000 (208.098)	Loss 1.0900 (1.0368)	Prec@1 71.289 (74.551)	Prec@5 94.727 (91.367)
AA Test: [19/98]	Time 0.000 (208.098)	Loss 1.0900 (1.0368)	Prec@1 71.289 (74.551)	Prec@5 94.727 (91.367)
[2025/01/31 09:01:06] - AA Test: [20/98]	Time 0.000 (198.204)	Loss 1.1629 (1.0428)	Prec@1 70.117 (74.340)	Prec@5 93.359 (91.462)
AA Test: [20/98]	Time 0.000 (198.204)	Loss 1.1629 (1.0428)	Prec@1 70.117 (74.340)	Prec@5 93.359 (91.462)
[2025/01/31 09:01:06] - AA Test: [21/98]	Time 0.000 (189.211)	Loss 1.0798 (1.0445)	Prec@1 73.633 (74.308)	Prec@5 91.406 (91.460)
AA Test: [21/98]	Time 0.000 (189.211)	Loss 1.0798 (1.0445)	Prec@1 73.633 (74.308)	Prec@5 91.406 (91.460)
[2025/01/31 09:01:07] - AA Test: [22/98]	Time 0.000 (181.004)	Loss 1.2980 (1.0555)	Prec@1 67.383 (74.006)	Prec@5 89.844 (91.389)
AA Test: [22/98]	Time 0.000 (181.004)	Loss 1.2980 (1.0555)	Prec@1 67.383 (74.006)	Prec@5 89.844 (91.389)
[2025/01/31 09:01:10] - AA Test: [23/98]	Time 0.000 (173.512)	Loss 0.9591 (1.0515)	Prec@1 70.898 (73.877)	Prec@5 95.117 (91.545)
AA Test: [23/98]	Time 0.000 (173.512)	Loss 0.9591 (1.0515)	Prec@1 70.898 (73.877)	Prec@5 95.117 (91.545)
[2025/01/31 09:01:10] - AA Test: [24/98]	Time 0.000 (166.588)	Loss 0.8821 (1.0447)	Prec@1 75.586 (73.945)	Prec@5 94.141 (91.648)
AA Test: [24/98]	Time 0.000 (166.588)	Loss 0.8821 (1.0447)	Prec@1 75.586 (73.945)	Prec@5 94.141 (91.648)
[2025/01/31 09:01:11] - AA Test: [25/98]	Time 0.000 (160.194)	Loss 0.9545 (1.0413)	Prec@1 73.047 (73.911)	Prec@5 94.141 (91.744)
AA Test: [25/98]	Time 0.000 (160.194)	Loss 0.9545 (1.0413)	Prec@1 73.047 (73.911)	Prec@5 94.141 (91.744)
[2025/01/31 09:01:13] - AA Test: [26/98]	Time 0.000 (154.302)	Loss 1.0985 (1.0434)	Prec@1 72.461 (73.857)	Prec@5 91.406 (91.732)
AA Test: [26/98]	Time 0.000 (154.302)	Loss 1.0985 (1.0434)	Prec@1 72.461 (73.857)	Prec@5 91.406 (91.732)
[2025/01/31 09:01:14] - AA Test: [27/98]	Time 0.000 (148.806)	Loss 1.1514 (1.0472)	Prec@1 66.016 (73.577)	Prec@5 92.969 (91.776)
AA Test: [27/98]	Time 0.000 (148.806)	Loss 1.1514 (1.0472)	Prec@1 66.016 (73.577)	Prec@5 92.969 (91.776)
[2025/01/31 09:01:15] - AA Test: [28/98]	Time 0.000 (143.689)	Loss 0.6626 (1.0340)	Prec@1 85.156 (73.976)	Prec@5 96.484 (91.938)
AA Test: [28/98]	Time 0.000 (143.689)	Loss 0.6626 (1.0340)	Prec@1 85.156 (73.976)	Prec@5 96.484 (91.938)
[2025/01/31 09:01:16] - AA Test: [29/98]	Time 0.000 (138.914)	Loss 0.9403 (1.0308)	Prec@1 75.781 (74.036)	Prec@5 94.727 (92.031)
AA Test: [29/98]	Time 0.000 (138.914)	Loss 0.9403 (1.0308)	Prec@1 75.781 (74.036)	Prec@5 94.727 (92.031)
[2025/01/31 09:01:17] - AA Test: [30/98]	Time 0.000 (134.445)	Loss 1.2652 (1.0384)	Prec@1 67.188 (73.816)	Prec@5 89.258 (91.942)
AA Test: [30/98]	Time 0.000 (134.445)	Loss 1.2652 (1.0384)	Prec@1 67.188 (73.816)	Prec@5 89.258 (91.942)
[2025/01/31 09:12:20] - AA Test: [31/98]	Time 0.000 (140.615)	Loss 0.4910 (1.0213)	Prec@1 87.305 (74.237)	Prec@5 96.875 (92.096)
AA Test: [31/98]	Time 0.000 (140.615)	Loss 0.4910 (1.0213)	Prec@1 87.305 (74.237)	Prec@5 96.875 (92.096)
[2025/01/31 09:23:25] - AA Test: [32/98]	Time 0.000 (146.430)	Loss 0.9326 (1.0186)	Prec@1 78.320 (74.361)	Prec@5 92.773 (92.116)
AA Test: [32/98]	Time 0.000 (146.430)	Loss 0.9326 (1.0186)	Prec@1 78.320 (74.361)	Prec@5 92.773 (92.116)
[2025/01/31 09:23:26] - AA Test: [33/98]	Time 0.000 (142.137)	Loss 0.9887 (1.0177)	Prec@1 74.805 (74.374)	Prec@5 91.797 (92.107)
AA Test: [33/98]	Time 0.000 (142.137)	Loss 0.9887 (1.0177)	Prec@1 74.805 (74.374)	Prec@5 91.797 (92.107)
[2025/01/31 09:23:27] - AA Test: [34/98]	Time 0.000 (138.088)	Loss 0.9457 (1.0157)	Prec@1 72.461 (74.319)	Prec@5 94.141 (92.165)
AA Test: [34/98]	Time 0.000 (138.088)	Loss 0.9457 (1.0157)	Prec@1 72.461 (74.319)	Prec@5 94.141 (92.165)
[2025/01/31 09:23:28] - AA Test: [35/98]	Time 0.000 (134.262)	Loss 0.9727 (1.0145)	Prec@1 75.781 (74.360)	Prec@5 92.578 (92.177)
AA Test: [35/98]	Time 0.000 (134.262)	Loss 0.9727 (1.0145)	Prec@1 75.781 (74.360)	Prec@5 92.578 (92.177)
[2025/01/31 09:23:29] - AA Test: [36/98]	Time 0.000 (130.643)	Loss 1.1585 (1.0184)	Prec@1 75.586 (74.393)	Prec@5 89.062 (92.092)
AA Test: [36/98]	Time 0.000 (130.643)	Loss 1.1585 (1.0184)	Prec@1 75.586 (74.393)	Prec@5 89.062 (92.092)
[2025/01/31 09:23:29] - AA Test: [37/98]	Time 0.000 (127.215)	Loss 1.0936 (1.0203)	Prec@1 71.875 (74.327)	Prec@5 91.992 (92.090)
AA Test: [37/98]	Time 0.000 (127.215)	Loss 1.0936 (1.0203)	Prec@1 71.875 (74.327)	Prec@5 91.992 (92.090)
[2025/01/31 09:34:29] - AA Test: [38/98]	Time 0.000 (132.415)	Loss 1.1328 (1.0232)	Prec@1 75.586 (74.359)	Prec@5 89.648 (92.027)
AA Test: [38/98]	Time 0.000 (132.415)	Loss 1.1328 (1.0232)	Prec@1 75.586 (74.359)	Prec@5 89.648 (92.027)
[2025/01/31 10:01:45] - AA Test: [39/98]	Time 0.000 (149.544)	Loss 1.1983 (1.0276)	Prec@1 69.727 (74.243)	Prec@5 90.039 (91.978)
AA Test: [39/98]	Time 0.000 (149.544)	Loss 1.1983 (1.0276)	Prec@1 69.727 (74.243)	Prec@5 90.039 (91.978)
[2025/01/31 10:34:26] - AA Test: [40/98]	Time 0.000 (169.822)	Loss 1.8814 (1.0484)	Prec@1 59.375 (73.881)	Prec@5 81.445 (91.721)
AA Test: [40/98]	Time 0.000 (169.822)	Loss 1.8814 (1.0484)	Prec@1 59.375 (73.881)	Prec@5 81.445 (91.721)
[2025/01/31 10:34:27] - AA Test: [41/98]	Time 0.000 (165.787)	Loss 1.3014 (1.0545)	Prec@1 67.383 (73.726)	Prec@5 88.477 (91.643)
AA Test: [41/98]	Time 0.000 (165.787)	Loss 1.3014 (1.0545)	Prec@1 67.383 (73.726)	Prec@5 88.477 (91.643)
[2025/01/31 10:34:28] - AA Test: [42/98]	Time 0.000 (161.942)	Loss 1.5821 (1.0667)	Prec@1 62.305 (73.460)	Prec@5 85.156 (91.493)
AA Test: [42/98]	Time 0.000 (161.942)	Loss 1.5821 (1.0667)	Prec@1 62.305 (73.460)	Prec@5 85.156 (91.493)
[2025/01/31 11:07:30] - AA Test: [43/98]	Time 0.000 (180.780)	Loss 1.3167 (1.0724)	Prec@1 68.359 (73.344)	Prec@5 86.914 (91.388)
AA Test: [43/98]	Time 0.000 (180.780)	Loss 1.3167 (1.0724)	Prec@1 68.359 (73.344)	Prec@5 86.914 (91.388)
[2025/01/31 11:07:31] - AA Test: [44/98]	Time 0.000 (176.776)	Loss 1.5834 (1.0838)	Prec@1 62.500 (73.103)	Prec@5 84.766 (91.241)
AA Test: [44/98]	Time 0.000 (176.776)	Loss 1.5834 (1.0838)	Prec@1 62.500 (73.103)	Prec@5 84.766 (91.241)
[2025/01/31 11:34:40] - AA Test: [45/98]	Time 0.000 (190.644)	Loss 2.0278 (1.1043)	Prec@1 54.297 (72.694)	Prec@5 78.711 (90.969)
AA Test: [45/98]	Time 0.000 (190.644)	Loss 2.0278 (1.1043)	Prec@1 54.297 (72.694)	Prec@5 78.711 (90.969)
[2025/01/31 11:45:44] - AA Test: [46/98]	Time 0.000 (193.649)	Loss 1.4920 (1.1125)	Prec@1 66.406 (72.561)	Prec@5 85.547 (90.854)
AA Test: [46/98]	Time 0.000 (193.649)	Loss 1.4920 (1.1125)	Prec@1 66.406 (72.561)	Prec@5 85.547 (90.854)
[2025/01/31 11:45:46] - AA Test: [47/98]	Time 0.000 (189.632)	Loss 1.7166 (1.1251)	Prec@1 58.008 (72.257)	Prec@5 84.570 (90.723)
AA Test: [47/98]	Time 0.000 (189.632)	Loss 1.7166 (1.1251)	Prec@1 58.008 (72.257)	Prec@5 84.570 (90.723)
[2025/01/31 11:45:47] - AA Test: [48/98]	Time 0.000 (185.771)	Loss 1.8431 (1.1398)	Prec@1 59.180 (71.991)	Prec@5 78.906 (90.482)
AA Test: [48/98]	Time 0.000 (185.771)	Loss 1.8431 (1.1398)	Prec@1 59.180 (71.991)	Prec@5 78.906 (90.482)
[2025/01/31 11:45:47] - AA Test: [49/98]	Time 0.000 (182.062)	Loss 1.7305 (1.1516)	Prec@1 58.008 (71.711)	Prec@5 83.594 (90.344)
AA Test: [49/98]	Time 0.000 (182.062)	Loss 1.7305 (1.1516)	Prec@1 58.008 (71.711)	Prec@5 83.594 (90.344)
[2025/01/31 11:56:39] - AA Test: [50/98]	Time 0.000 (184.880)	Loss 1.3904 (1.1563)	Prec@1 65.625 (71.592)	Prec@5 86.914 (90.277)
AA Test: [50/98]	Time 0.000 (184.880)	Loss 1.3904 (1.1563)	Prec@1 65.625 (71.592)	Prec@5 86.914 (90.277)
[2025/01/31 12:26:30] - AA Test: [51/98]	Time 0.000 (198.551)	Loss 1.7023 (1.1668)	Prec@1 59.375 (71.357)	Prec@5 84.570 (90.167)
AA Test: [51/98]	Time 0.000 (198.551)	Loss 1.7023 (1.1668)	Prec@1 59.375 (71.357)	Prec@5 84.570 (90.167)
[2025/01/31 12:53:39] - AA Test: [52/98]	Time 0.000 (210.172)	Loss 1.5186 (1.1734)	Prec@1 65.234 (71.241)	Prec@5 83.203 (90.035)
AA Test: [52/98]	Time 0.000 (210.172)	Loss 1.5186 (1.1734)	Prec@1 65.234 (71.241)	Prec@5 83.203 (90.035)
[2025/01/31 13:20:51] - AA Test: [53/98]	Time 0.000 (221.388)	Loss 1.3546 (1.1768)	Prec@1 64.844 (71.123)	Prec@5 86.914 (89.978)
AA Test: [53/98]	Time 0.000 (221.388)	Loss 1.3546 (1.1768)	Prec@1 64.844 (71.123)	Prec@5 86.914 (89.978)
[2025/01/31 13:50:43] - AA Test: [54/98]	Time 0.000 (233.659)	Loss 1.2241 (1.1776)	Prec@1 74.805 (71.190)	Prec@5 86.523 (89.915)
AA Test: [54/98]	Time 0.000 (233.659)	Loss 1.2241 (1.1776)	Prec@1 74.805 (71.190)	Prec@5 86.523 (89.915)
[2025/01/31 13:50:45] - AA Test: [55/98]	Time 0.000 (229.496)	Loss 1.0832 (1.1759)	Prec@1 74.609 (71.251)	Prec@5 90.234 (89.920)
AA Test: [55/98]	Time 0.000 (229.496)	Loss 1.0832 (1.1759)	Prec@1 74.609 (71.251)	Prec@5 90.234 (89.920)
[2025/01/31 13:50:46] - AA Test: [56/98]	Time 0.000 (225.484)	Loss 1.0351 (1.1735)	Prec@1 74.609 (71.310)	Prec@5 91.406 (89.947)
AA Test: [56/98]	Time 0.000 (225.484)	Loss 1.0351 (1.1735)	Prec@1 74.609 (71.310)	Prec@5 91.406 (89.947)
[2025/01/31 14:20:38] - AA Test: [57/98]	Time 0.000 (237.045)	Loss 1.9241 (1.1864)	Prec@1 59.180 (71.100)	Prec@5 78.711 (89.753)
AA Test: [57/98]	Time 0.000 (237.045)	Loss 1.9241 (1.1864)	Prec@1 59.180 (71.100)	Prec@5 78.711 (89.753)
[2025/01/31 14:53:38] - AA Test: [58/98]	Time 0.000 (249.803)	Loss 1.6543 (1.1943)	Prec@1 64.258 (70.985)	Prec@5 82.422 (89.629)
AA Test: [58/98]	Time 0.000 (249.803)	Loss 1.6543 (1.1943)	Prec@1 64.258 (70.985)	Prec@5 82.422 (89.629)
[2025/01/31 15:20:51] - AA Test: [59/98]	Time 0.000 (259.250)	Loss 1.0757 (1.1924)	Prec@1 75.977 (71.068)	Prec@5 88.086 (89.603)
AA Test: [59/98]	Time 0.000 (259.250)	Loss 1.0757 (1.1924)	Prec@1 75.977 (71.068)	Prec@5 88.086 (89.603)
[2025/01/31 15:20:52] - AA Test: [60/98]	Time 0.000 (255.006)	Loss 2.0586 (1.2066)	Prec@1 53.125 (70.774)	Prec@5 77.734 (89.408)
AA Test: [60/98]	Time 0.000 (255.006)	Loss 2.0586 (1.2066)	Prec@1 53.125 (70.774)	Prec@5 77.734 (89.408)
[2025/01/31 15:20:53] - AA Test: [61/98]	Time 0.000 (250.901)	Loss 1.6202 (1.2132)	Prec@1 66.211 (70.700)	Prec@5 83.789 (89.318)
AA Test: [61/98]	Time 0.000 (250.901)	Loss 1.6202 (1.2132)	Prec@1 66.211 (70.700)	Prec@5 83.789 (89.318)
[2025/01/31 15:48:01] - AA Test: [62/98]	Time 0.000 (259.844)	Loss 1.4364 (1.2168)	Prec@1 58.984 (70.514)	Prec@5 86.719 (89.276)
AA Test: [62/98]	Time 0.000 (259.844)	Loss 1.4364 (1.2168)	Prec@1 58.984 (70.514)	Prec@5 86.719 (89.276)
[2025/01/31 15:48:02] - AA Test: [63/98]	Time 0.000 (255.791)	Loss 1.6709 (1.2239)	Prec@1 65.234 (70.432)	Prec@5 84.766 (89.206)
AA Test: [63/98]	Time 0.000 (255.791)	Loss 1.6709 (1.2239)	Prec@1 65.234 (70.432)	Prec@5 84.766 (89.206)
[2025/01/31 15:48:04] - AA Test: [64/98]	Time 0.000 (251.865)	Loss 1.7326 (1.2317)	Prec@1 54.688 (70.189)	Prec@5 85.352 (89.147)
AA Test: [64/98]	Time 0.000 (251.865)	Loss 1.7326 (1.2317)	Prec@1 54.688 (70.189)	Prec@5 85.352 (89.147)
[2025/01/31 15:59:08] - AA Test: [65/98]	Time 0.000 (253.081)	Loss 1.3160 (1.2330)	Prec@1 68.750 (70.167)	Prec@5 85.352 (89.089)
AA Test: [65/98]	Time 0.000 (253.081)	Loss 1.3160 (1.2330)	Prec@1 68.750 (70.167)	Prec@5 85.352 (89.089)
[2025/01/31 16:32:02] - AA Test: [66/98]	Time 0.000 (264.038)	Loss 1.5112 (1.2371)	Prec@1 66.797 (70.117)	Prec@5 84.180 (89.016)
AA Test: [66/98]	Time 0.000 (264.038)	Loss 1.5112 (1.2371)	Prec@1 66.797 (70.117)	Prec@5 84.180 (89.016)
[2025/01/31 16:43:07] - AA Test: [67/98]	Time 0.000 (265.042)	Loss 1.6840 (1.2437)	Prec@1 60.547 (69.976)	Prec@5 82.031 (88.913)
AA Test: [67/98]	Time 0.000 (265.042)	Loss 1.6840 (1.2437)	Prec@1 60.547 (69.976)	Prec@5 82.031 (88.913)
[2025/01/31 17:10:23] - AA Test: [68/98]	Time 0.000 (273.057)	Loss 1.5489 (1.2481)	Prec@1 65.820 (69.916)	Prec@5 84.375 (88.847)
AA Test: [68/98]	Time 0.000 (273.057)	Loss 1.5489 (1.2481)	Prec@1 65.820 (69.916)	Prec@5 84.375 (88.847)
[2025/01/31 17:43:48] - AA Test: [69/98]	Time 0.000 (283.477)	Loss 1.4723 (1.2513)	Prec@1 67.969 (69.888)	Prec@5 85.742 (88.803)
AA Test: [69/98]	Time 0.000 (283.477)	Loss 1.4723 (1.2513)	Prec@1 67.969 (69.888)	Prec@5 85.742 (88.803)
[2025/01/31 17:43:49] - AA Test: [70/98]	Time 0.000 (279.491)	Loss 1.2815 (1.2517)	Prec@1 71.484 (69.911)	Prec@5 87.305 (88.782)
AA Test: [70/98]	Time 0.000 (279.491)	Loss 1.2815 (1.2517)	Prec@1 71.484 (69.911)	Prec@5 87.305 (88.782)
[2025/01/31 17:54:53] - AA Test: [71/98]	Time 0.000 (280.221)	Loss 1.8763 (1.2604)	Prec@1 60.742 (69.784)	Prec@5 78.125 (88.634)
AA Test: [71/98]	Time 0.000 (280.221)	Loss 1.8763 (1.2604)	Prec@1 60.742 (69.784)	Prec@5 78.125 (88.634)
[2025/01/31 17:54:53] - AA Test: [72/98]	Time 0.000 (276.386)	Loss 1.8295 (1.2682)	Prec@1 54.102 (69.569)	Prec@5 79.688 (88.511)
AA Test: [72/98]	Time 0.000 (276.386)	Loss 1.8295 (1.2682)	Prec@1 54.102 (69.569)	Prec@5 79.688 (88.511)
[2025/01/31 18:25:03] - AA Test: [73/98]	Time 0.000 (284.879)	Loss 1.6107 (1.2728)	Prec@1 62.695 (69.476)	Prec@5 84.375 (88.455)
AA Test: [73/98]	Time 0.000 (284.879)	Loss 1.6107 (1.2728)	Prec@1 62.695 (69.476)	Prec@5 84.375 (88.455)
[2025/01/31 18:36:08] - AA Test: [74/98]	Time 0.000 (285.516)	Loss 1.8022 (1.2799)	Prec@1 63.281 (69.393)	Prec@5 82.617 (88.378)
AA Test: [74/98]	Time 0.000 (285.516)	Loss 1.8022 (1.2799)	Prec@1 63.281 (69.393)	Prec@5 82.617 (88.378)
[2025/01/31 19:06:01] - AA Test: [75/98]	Time 0.000 (293.550)	Loss 1.5791 (1.2838)	Prec@1 67.578 (69.369)	Prec@5 82.617 (88.302)
AA Test: [75/98]	Time 0.000 (293.550)	Loss 1.5791 (1.2838)	Prec@1 67.578 (69.369)	Prec@5 82.617 (88.302)
[2025/01/31 19:17:01] - AA Test: [76/98]	Time 0.000 (294.024)	Loss 1.5743 (1.2876)	Prec@1 65.430 (69.318)	Prec@5 82.812 (88.231)
AA Test: [76/98]	Time 0.000 (294.024)	Loss 1.5743 (1.2876)	Prec@1 65.430 (69.318)	Prec@5 82.812 (88.231)
[2025/01/31 19:49:54] - AA Test: [77/98]	Time 0.000 (302.905)	Loss 2.0487 (1.2974)	Prec@1 55.469 (69.141)	Prec@5 74.414 (88.053)
AA Test: [77/98]	Time 0.000 (302.905)	Loss 2.0487 (1.2974)	Prec@1 55.469 (69.141)	Prec@5 74.414 (88.053)
[2025/01/31 20:01:02] - AA Test: [78/98]	Time 0.000 (303.294)	Loss 1.3410 (1.2979)	Prec@1 71.289 (69.168)	Prec@5 86.523 (88.034)
AA Test: [78/98]	Time 0.000 (303.294)	Loss 1.3410 (1.2979)	Prec@1 71.289 (69.168)	Prec@5 86.523 (88.034)
[2025/01/31 20:31:10] - AA Test: [79/98]	Time 0.000 (310.805)	Loss 1.8536 (1.3049)	Prec@1 54.883 (68.989)	Prec@5 82.227 (87.961)
AA Test: [79/98]	Time 0.000 (310.805)	Loss 1.8536 (1.3049)	Prec@1 54.883 (68.989)	Prec@5 82.227 (87.961)
[2025/01/31 20:31:11] - AA Test: [80/98]	Time 0.000 (306.973)	Loss 1.7628 (1.3105)	Prec@1 60.742 (68.887)	Prec@5 82.617 (87.895)
AA Test: [80/98]	Time 0.000 (306.973)	Loss 1.7628 (1.3105)	Prec@1 60.742 (68.887)	Prec@5 82.617 (87.895)
[2025/01/31 21:01:02] - AA Test: [81/98]	Time 0.000 (314.155)	Loss 1.9749 (1.3186)	Prec@1 54.883 (68.717)	Prec@5 77.148 (87.764)
AA Test: [81/98]	Time 0.000 (314.155)	Loss 1.9749 (1.3186)	Prec@1 54.883 (68.717)	Prec@5 77.148 (87.764)
[2025/01/31 21:12:08] - AA Test: [82/98]	Time 0.000 (314.381)	Loss 1.8603 (1.3252)	Prec@1 54.688 (68.548)	Prec@5 82.031 (87.695)
AA Test: [82/98]	Time 0.000 (314.381)	Loss 1.8603 (1.3252)	Prec@1 54.688 (68.548)	Prec@5 82.031 (87.695)
[2025/01/31 21:23:11] - AA Test: [83/98]	Time 0.000 (314.582)	Loss 1.3360 (1.3253)	Prec@1 69.336 (68.557)	Prec@5 87.695 (87.695)
AA Test: [83/98]	Time 0.000 (314.582)	Loss 1.3360 (1.3253)	Prec@1 69.336 (68.557)	Prec@5 87.695 (87.695)
[2025/01/31 21:23:12] - AA Test: [84/98]	Time 0.000 (310.888)	Loss 1.6692 (1.3293)	Prec@1 62.109 (68.481)	Prec@5 84.375 (87.656)
AA Test: [84/98]	Time 0.000 (310.888)	Loss 1.6692 (1.3293)	Prec@1 62.109 (68.481)	Prec@5 84.375 (87.656)
[2025/01/31 21:50:25] - AA Test: [85/98]	Time 0.000 (316.768)	Loss 1.1522 (1.3273)	Prec@1 68.359 (68.480)	Prec@5 91.016 (87.695)
AA Test: [85/98]	Time 0.000 (316.768)	Loss 1.1522 (1.3273)	Prec@1 68.359 (68.480)	Prec@5 91.016 (87.695)
[2025/01/31 22:01:28] - AA Test: [86/98]	Time 0.000 (316.935)	Loss 1.6490 (1.3310)	Prec@1 61.719 (68.402)	Prec@5 81.445 (87.623)
AA Test: [86/98]	Time 0.000 (316.935)	Loss 1.6490 (1.3310)	Prec@1 61.719 (68.402)	Prec@5 81.445 (87.623)
[2025/01/31 22:28:42] - AA Test: [87/98]	Time 0.000 (322.620)	Loss 1.7483 (1.3357)	Prec@1 56.641 (68.268)	Prec@5 81.445 (87.553)
AA Test: [87/98]	Time 0.000 (322.620)	Loss 1.7483 (1.3357)	Prec@1 56.641 (68.268)	Prec@5 81.445 (87.553)
[2025/01/31 22:55:52] - AA Test: [88/98]	Time 0.000 (328.149)	Loss 2.0562 (1.3438)	Prec@1 51.758 (68.083)	Prec@5 78.320 (87.450)
AA Test: [88/98]	Time 0.000 (328.149)	Loss 2.0562 (1.3438)	Prec@1 51.758 (68.083)	Prec@5 78.320 (87.450)
[2025/01/31 22:55:52] - AA Test: [89/98]	Time 0.000 (324.507)	Loss 1.3070 (1.3434)	Prec@1 72.266 (68.129)	Prec@5 85.352 (87.426)
AA Test: [89/98]	Time 0.000 (324.507)	Loss 1.3070 (1.3434)	Prec@1 72.266 (68.129)	Prec@5 85.352 (87.426)
[2025/01/31 23:23:04] - AA Test: [90/98]	Time 0.000 (329.908)	Loss 1.6823 (1.3471)	Prec@1 58.203 (68.020)	Prec@5 84.180 (87.391)
AA Test: [90/98]	Time 0.000 (329.908)	Loss 1.6823 (1.3471)	Prec@1 58.203 (68.020)	Prec@5 84.180 (87.391)
[2025/01/31 23:23:05] - AA Test: [91/98]	Time 0.000 (326.324)	Loss 1.2042 (1.3456)	Prec@1 70.898 (68.052)	Prec@5 89.648 (87.415)
AA Test: [91/98]	Time 0.000 (326.324)	Loss 1.2042 (1.3456)	Prec@1 70.898 (68.052)	Prec@5 89.648 (87.415)
[2025/01/31 23:23:05] - AA Test: [92/98]	Time 0.000 (322.819)	Loss 1.1147 (1.3431)	Prec@1 71.289 (68.086)	Prec@5 91.602 (87.460)
AA Test: [92/98]	Time 0.000 (322.819)	Loss 1.1147 (1.3431)	Prec@1 71.289 (68.086)	Prec@5 91.602 (87.460)
[2025/01/31 23:23:06] - AA Test: [93/98]	Time 0.000 (319.389)	Loss 1.3956 (1.3436)	Prec@1 66.602 (68.071)	Prec@5 84.375 (87.427)
AA Test: [93/98]	Time 0.000 (319.389)	Loss 1.3956 (1.3436)	Prec@1 66.602 (68.071)	Prec@5 84.375 (87.427)
[2025/01/31 23:23:07] - AA Test: [94/98]	Time 0.000 (316.031)	Loss 1.9427 (1.3499)	Prec@1 57.617 (67.961)	Prec@5 80.664 (87.356)
AA Test: [94/98]	Time 0.000 (316.031)	Loss 1.9427 (1.3499)	Prec@1 57.617 (67.961)	Prec@5 80.664 (87.356)
[2025/01/31 23:23:11] - AA Test: [95/98]	Time 0.000 (312.758)	Loss 1.4592 (1.3511)	Prec@1 65.234 (67.932)	Prec@5 89.258 (87.376)
AA Test: [95/98]	Time 0.000 (312.758)	Loss 1.4592 (1.3511)	Prec@1 65.234 (67.932)	Prec@5 89.258 (87.376)
[2025/01/31 23:34:17] - AA Test: [96/98]	Time 0.000 (312.969)	Loss 0.5836 (1.3432)	Prec@1 84.180 (68.100)	Prec@5 95.703 (87.462)
AA Test: [96/98]	Time 0.000 (312.969)	Loss 0.5836 (1.3432)	Prec@1 84.180 (68.100)	Prec@5 95.703 (87.462)
[2025/01/31 23:34:18] - AA Test: [97/98]	Time 0.000 (309.778)	Loss 1.3458 (1.3432)	Prec@1 69.345 (68.108)	Prec@5 86.607 (87.456)
AA Test: [97/98]	Time 0.000 (309.778)	Loss 1.3458 (1.3432)	Prec@1 69.345 (68.108)	Prec@5 86.607 (87.456)
[2025/01/31 23:34:18] -  AA Mask Final Prec@1 68.108 Prec@5 87.456
 AA Mask Final Prec@1 68.108 Prec@5 87.456
[2025/01/31 23:34:18] - AA Mask Acc: 68.1080
AA Mask Acc: 68.1080
[2025/01/31 23:34:18] - *** PGD eps: 0.01568627450980392, K: 10, step: 0.00392156862745098 ***
*** PGD eps: 0.01568627450980392, K: 10, step: 0.00392156862745098 ***
[2025/01/31 23:34:26] - PGD Test: [0/98]	Time 7.955 (7.955)	Loss 1.6890 (1.6890)	Prec@1 58.789 (58.789)	Prec@5 84.961 (84.961)
PGD Test: [0/98]	Time 7.955 (7.955)	Loss 1.6890 (1.6890)	Prec@1 58.789 (58.789)	Prec@5 84.961 (84.961)
[2025/01/31 23:35:11] - PGD Test: [20/98]	Time 2.287 (2.561)	Loss 2.0293 (1.7633)	Prec@1 41.016 (56.194)	Prec@5 82.227 (83.166)
PGD Test: [20/98]	Time 2.287 (2.561)	Loss 2.0293 (1.7633)	Prec@1 41.016 (56.194)	Prec@5 82.227 (83.166)
[2025/01/31 23:35:57] - PGD Test: [40/98]	Time 2.286 (2.428)	Loss 4.0406 (1.7740)	Prec@1 24.805 (55.945)	Prec@5 54.492 (83.508)
PGD Test: [40/98]	Time 2.286 (2.428)	Loss 4.0406 (1.7740)	Prec@1 24.805 (55.945)	Prec@5 54.492 (83.508)
[2025/01/31 23:36:43] - PGD Test: [60/98]	Time 2.280 (2.381)	Loss 2.7259 (1.9382)	Prec@1 40.625 (53.775)	Prec@5 68.164 (80.911)
PGD Test: [60/98]	Time 2.280 (2.381)	Loss 2.7259 (1.9382)	Prec@1 40.625 (53.775)	Prec@5 68.164 (80.911)
[2025/01/31 23:37:29] - PGD Test: [80/98]	Time 2.281 (2.357)	Loss 2.3014 (2.0408)	Prec@1 49.219 (52.385)	Prec@5 74.805 (79.316)
PGD Test: [80/98]	Time 2.281 (2.357)	Loss 2.3014 (2.0408)	Prec@1 49.219 (52.385)	Prec@5 74.805 (79.316)
[2025/01/31 23:38:09] -  PGD Final Prec@1 51.686 Prec@5 78.618
 PGD Final Prec@1 51.686 Prec@5 78.618
/home/yxma/anaconda3/envs/py39_torch/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/yxma/anaconda3/envs/py39_torch/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:814: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
[2025/01/31 23:38:09] - epoch 84 lr 1.091954e-03
epoch 84 lr 1.091954e-03
[2025/01/31 23:38:16] - Train Epoch: [84][0/2503]	Time 4.544 (4.544)	Data 0.000 (0.000)	Loss 2.2202 (2.2202)	Prec@1 50.195 (50.195)	Prec@5 76.562 (76.562)
Train Epoch: [84][0/2503]	Time 4.544 (4.544)	Data 0.000 (0.000)	Loss 2.2202 (2.2202)	Prec@1 50.195 (50.195)	Prec@5 76.562 (76.562)
[2025/01/31 23:39:15] - Train Epoch: [84][20/2503]	Time 2.808 (2.893)	Data 0.000 (0.000)	Loss 2.0495 (1.9803)	Prec@1 53.516 (54.929)	Prec@5 80.273 (79.399)
Train Epoch: [84][20/2503]	Time 2.808 (2.893)	Data 0.000 (0.000)	Loss 2.0495 (1.9803)	Prec@1 53.516 (54.929)	Prec@5 80.273 (79.399)
[2025/01/31 23:40:14] - Train Epoch: [84][40/2503]	Time 2.801 (2.854)	Data 0.000 (0.000)	Loss 1.9143 (1.9604)	Prec@1 59.766 (55.497)	Prec@5 79.688 (79.302)
Train Epoch: [84][40/2503]	Time 2.801 (2.854)	Data 0.000 (0.000)	Loss 1.9143 (1.9604)	Prec@1 59.766 (55.497)	Prec@5 79.688 (79.302)
[2025/01/31 23:41:14] - Train Epoch: [84][60/2503]	Time 2.805 (2.840)	Data 0.000 (0.000)	Loss 2.0658 (1.9522)	Prec@1 53.906 (55.510)	Prec@5 77.148 (79.217)
Train Epoch: [84][60/2503]	Time 2.805 (2.840)	Data 0.000 (0.000)	Loss 2.0658 (1.9522)	Prec@1 53.906 (55.510)	Prec@5 77.148 (79.217)
[2025/01/31 23:42:13] - Train Epoch: [84][80/2503]	Time 2.804 (2.834)	Data 0.000 (0.000)	Loss 1.9204 (1.9578)	Prec@1 58.594 (55.490)	Prec@5 79.297 (79.150)
Train Epoch: [84][80/2503]	Time 2.804 (2.834)	Data 0.000 (0.000)	Loss 1.9204 (1.9578)	Prec@1 58.594 (55.490)	Prec@5 79.297 (79.150)
[2025/01/31 23:43:12] - Train Epoch: [84][100/2503]	Time 2.804 (2.830)	Data 0.000 (0.000)	Loss 1.9337 (1.9607)	Prec@1 56.641 (55.442)	Prec@5 79.297 (79.138)
Train Epoch: [84][100/2503]	Time 2.804 (2.830)	Data 0.000 (0.000)	Loss 1.9337 (1.9607)	Prec@1 56.641 (55.442)	Prec@5 79.297 (79.138)
[2025/01/31 23:44:11] - Train Epoch: [84][120/2503]	Time 2.809 (2.827)	Data 0.000 (0.000)	Loss 1.9195 (1.9571)	Prec@1 58.008 (55.574)	Prec@5 79.688 (79.231)
Train Epoch: [84][120/2503]	Time 2.809 (2.827)	Data 0.000 (0.000)	Loss 1.9195 (1.9571)	Prec@1 58.008 (55.574)	Prec@5 79.688 (79.231)
[2025/01/31 23:45:11] - Train Epoch: [84][140/2503]	Time 2.803 (2.826)	Data 0.000 (0.000)	Loss 2.0359 (1.9575)	Prec@1 52.930 (55.550)	Prec@5 78.516 (79.282)
Train Epoch: [84][140/2503]	Time 2.803 (2.826)	Data 0.000 (0.000)	Loss 2.0359 (1.9575)	Prec@1 52.930 (55.550)	Prec@5 78.516 (79.282)
[2025/01/31 23:46:10] - Train Epoch: [84][160/2503]	Time 2.804 (2.824)	Data 0.000 (0.000)	Loss 1.9619 (1.9569)	Prec@1 54.492 (55.499)	Prec@5 79.102 (79.305)
Train Epoch: [84][160/2503]	Time 2.804 (2.824)	Data 0.000 (0.000)	Loss 1.9619 (1.9569)	Prec@1 54.492 (55.499)	Prec@5 79.102 (79.305)
[2025/01/31 23:47:09] - Train Epoch: [84][180/2503]	Time 2.808 (2.823)	Data 0.000 (0.000)	Loss 1.8408 (1.9547)	Prec@1 57.617 (55.530)	Prec@5 81.836 (79.311)
Train Epoch: [84][180/2503]	Time 2.808 (2.823)	Data 0.000 (0.000)	Loss 1.8408 (1.9547)	Prec@1 57.617 (55.530)	Prec@5 81.836 (79.311)
[2025/01/31 23:48:09] - Train Epoch: [84][200/2503]	Time 2.809 (2.822)	Data 0.000 (0.000)	Loss 2.0500 (1.9512)	Prec@1 54.883 (55.621)	Prec@5 76.562 (79.372)
Train Epoch: [84][200/2503]	Time 2.809 (2.822)	Data 0.000 (0.000)	Loss 2.0500 (1.9512)	Prec@1 54.883 (55.621)	Prec@5 76.562 (79.372)
[2025/01/31 23:49:08] - Train Epoch: [84][220/2503]	Time 2.809 (2.822)	Data 0.000 (0.000)	Loss 1.8512 (1.9541)	Prec@1 57.422 (55.612)	Prec@5 81.641 (79.330)
Train Epoch: [84][220/2503]	Time 2.809 (2.822)	Data 0.000 (0.000)	Loss 1.8512 (1.9541)	Prec@1 57.422 (55.612)	Prec@5 81.641 (79.330)
[2025/01/31 23:50:07] - Train Epoch: [84][240/2503]	Time 2.805 (2.821)	Data 0.000 (0.000)	Loss 1.7911 (1.9534)	Prec@1 56.250 (55.655)	Prec@5 81.836 (79.347)
Train Epoch: [84][240/2503]	Time 2.805 (2.821)	Data 0.000 (0.000)	Loss 1.7911 (1.9534)	Prec@1 56.250 (55.655)	Prec@5 81.836 (79.347)
[2025/01/31 23:51:07] - Train Epoch: [84][260/2503]	Time 2.803 (2.820)	Data 0.000 (0.000)	Loss 1.9296 (1.9563)	Prec@1 56.055 (55.636)	Prec@5 78.516 (79.285)
Train Epoch: [84][260/2503]	Time 2.803 (2.820)	Data 0.000 (0.000)	Loss 1.9296 (1.9563)	Prec@1 56.055 (55.636)	Prec@5 78.516 (79.285)
[2025/01/31 23:52:06] - Train Epoch: [84][280/2503]	Time 2.805 (2.820)	Data 0.000 (0.000)	Loss 1.8808 (1.9533)	Prec@1 57.031 (55.648)	Prec@5 79.883 (79.326)
Train Epoch: [84][280/2503]	Time 2.805 (2.820)	Data 0.000 (0.000)	Loss 1.8808 (1.9533)	Prec@1 57.031 (55.648)	Prec@5 79.883 (79.326)
[2025/01/31 23:53:05] - Train Epoch: [84][300/2503]	Time 2.808 (2.820)	Data 0.000 (0.000)	Loss 1.8444 (1.9516)	Prec@1 57.227 (55.671)	Prec@5 82.031 (79.362)
Train Epoch: [84][300/2503]	Time 2.808 (2.820)	Data 0.000 (0.000)	Loss 1.8444 (1.9516)	Prec@1 57.227 (55.671)	Prec@5 82.031 (79.362)
[2025/01/31 23:54:05] - Train Epoch: [84][320/2503]	Time 2.807 (2.819)	Data 0.000 (0.000)	Loss 2.0767 (1.9524)	Prec@1 53.516 (55.667)	Prec@5 78.906 (79.353)
Train Epoch: [84][320/2503]	Time 2.807 (2.819)	Data 0.000 (0.000)	Loss 2.0767 (1.9524)	Prec@1 53.516 (55.667)	Prec@5 78.906 (79.353)
[2025/01/31 23:55:04] - Train Epoch: [84][340/2503]	Time 2.806 (2.819)	Data 0.000 (0.000)	Loss 2.2017 (1.9512)	Prec@1 49.023 (55.677)	Prec@5 75.977 (79.376)
Train Epoch: [84][340/2503]	Time 2.806 (2.819)	Data 0.000 (0.000)	Loss 2.2017 (1.9512)	Prec@1 49.023 (55.677)	Prec@5 75.977 (79.376)
[2025/01/31 23:56:03] - Train Epoch: [84][360/2503]	Time 2.803 (2.819)	Data 0.000 (0.000)	Loss 1.6815 (1.9509)	Prec@1 61.523 (55.672)	Prec@5 82.227 (79.374)
Train Epoch: [84][360/2503]	Time 2.803 (2.819)	Data 0.000 (0.000)	Loss 1.6815 (1.9509)	Prec@1 61.523 (55.672)	Prec@5 82.227 (79.374)
[2025/01/31 23:57:03] - Train Epoch: [84][380/2503]	Time 2.805 (2.819)	Data 0.000 (0.000)	Loss 2.3240 (1.9525)	Prec@1 48.828 (55.650)	Prec@5 74.609 (79.369)
Train Epoch: [84][380/2503]	Time 2.805 (2.819)	Data 0.000 (0.000)	Loss 2.3240 (1.9525)	Prec@1 48.828 (55.650)	Prec@5 74.609 (79.369)
[2025/01/31 23:58:02] - Train Epoch: [84][400/2503]	Time 2.798 (2.818)	Data 0.000 (0.000)	Loss 1.7422 (1.9526)	Prec@1 59.180 (55.627)	Prec@5 84.375 (79.364)
Train Epoch: [84][400/2503]	Time 2.798 (2.818)	Data 0.000 (0.000)	Loss 1.7422 (1.9526)	Prec@1 59.180 (55.627)	Prec@5 84.375 (79.364)
[2025/01/31 23:59:01] - Train Epoch: [84][420/2503]	Time 2.815 (2.818)	Data 0.000 (0.000)	Loss 2.0062 (1.9536)	Prec@1 54.688 (55.632)	Prec@5 79.492 (79.361)
Train Epoch: [84][420/2503]	Time 2.815 (2.818)	Data 0.000 (0.000)	Loss 2.0062 (1.9536)	Prec@1 54.688 (55.632)	Prec@5 79.492 (79.361)
[2025/02/01 00:00:01] - Train Epoch: [84][440/2503]	Time 2.806 (2.818)	Data 0.000 (0.000)	Loss 1.7880 (1.9547)	Prec@1 59.180 (55.610)	Prec@5 79.688 (79.352)
Train Epoch: [84][440/2503]	Time 2.806 (2.818)	Data 0.000 (0.000)	Loss 1.7880 (1.9547)	Prec@1 59.180 (55.610)	Prec@5 79.688 (79.352)
[2025/02/01 00:01:00] - Train Epoch: [84][460/2503]	Time 2.804 (2.818)	Data 0.000 (0.000)	Loss 2.0621 (1.9553)	Prec@1 53.906 (55.611)	Prec@5 78.516 (79.341)
Train Epoch: [84][460/2503]	Time 2.804 (2.818)	Data 0.000 (0.000)	Loss 2.0621 (1.9553)	Prec@1 53.906 (55.611)	Prec@5 78.516 (79.341)
[2025/02/01 00:01:59] - Train Epoch: [84][480/2503]	Time 2.806 (2.818)	Data 0.000 (0.000)	Loss 1.9589 (1.9569)	Prec@1 54.492 (55.554)	Prec@5 81.055 (79.338)
Train Epoch: [84][480/2503]	Time 2.806 (2.818)	Data 0.000 (0.000)	Loss 1.9589 (1.9569)	Prec@1 54.492 (55.554)	Prec@5 81.055 (79.338)
[2025/02/01 00:02:59] - Train Epoch: [84][500/2503]	Time 2.810 (2.818)	Data 0.000 (0.000)	Loss 1.9574 (1.9567)	Prec@1 54.102 (55.562)	Prec@5 80.469 (79.337)
Train Epoch: [84][500/2503]	Time 2.810 (2.818)	Data 0.000 (0.000)	Loss 1.9574 (1.9567)	Prec@1 54.102 (55.562)	Prec@5 80.469 (79.337)
[2025/02/01 00:04:00] - Train Epoch: [84][520/2503]	Time 2.805 (2.817)	Data 0.000 (0.000)	Loss 1.7252 (1.9590)	Prec@1 59.570 (55.538)	Prec@5 82.227 (79.305)
Train Epoch: [84][520/2503]	Time 2.805 (2.817)	Data 0.000 (0.000)	Loss 1.7252 (1.9590)	Prec@1 59.570 (55.538)	Prec@5 82.227 (79.305)
[2025/02/01 00:04:59] - Train Epoch: [84][540/2503]	Time 2.816 (2.817)	Data 0.000 (0.000)	Loss 1.8644 (1.9588)	Prec@1 59.375 (55.538)	Prec@5 82.422 (79.313)
Train Epoch: [84][540/2503]	Time 2.816 (2.817)	Data 0.000 (0.000)	Loss 1.8644 (1.9588)	Prec@1 59.375 (55.538)	Prec@5 82.422 (79.313)
[2025/02/01 00:05:58] - Train Epoch: [84][560/2503]	Time 2.807 (2.817)	Data 0.000 (0.000)	Loss 2.0441 (1.9610)	Prec@1 51.758 (55.498)	Prec@5 76.758 (79.275)
Train Epoch: [84][560/2503]	Time 2.807 (2.817)	Data 0.000 (0.000)	Loss 2.0441 (1.9610)	Prec@1 51.758 (55.498)	Prec@5 76.758 (79.275)
[2025/02/01 00:06:58] - Train Epoch: [84][580/2503]	Time 2.802 (2.817)	Data 0.000 (0.000)	Loss 1.9127 (1.9616)	Prec@1 57.812 (55.480)	Prec@5 79.883 (79.259)
Train Epoch: [84][580/2503]	Time 2.802 (2.817)	Data 0.000 (0.000)	Loss 1.9127 (1.9616)	Prec@1 57.812 (55.480)	Prec@5 79.883 (79.259)
[2025/02/01 00:07:57] - Train Epoch: [84][600/2503]	Time 2.805 (2.816)	Data 0.000 (0.000)	Loss 1.9659 (1.9617)	Prec@1 58.594 (55.470)	Prec@5 77.344 (79.262)
Train Epoch: [84][600/2503]	Time 2.805 (2.816)	Data 0.000 (0.000)	Loss 1.9659 (1.9617)	Prec@1 58.594 (55.470)	Prec@5 77.344 (79.262)
[2025/02/01 00:08:56] - Train Epoch: [84][620/2503]	Time 2.807 (2.816)	Data 0.000 (0.000)	Loss 1.9710 (1.9638)	Prec@1 54.688 (55.422)	Prec@5 80.469 (79.228)
Train Epoch: [84][620/2503]	Time 2.807 (2.816)	Data 0.000 (0.000)	Loss 1.9710 (1.9638)	Prec@1 54.688 (55.422)	Prec@5 80.469 (79.228)
[2025/02/01 00:09:55] - Train Epoch: [84][640/2503]	Time 2.809 (2.816)	Data 0.000 (0.000)	Loss 2.0655 (1.9646)	Prec@1 53.906 (55.385)	Prec@5 79.102 (79.215)
Train Epoch: [84][640/2503]	Time 2.809 (2.816)	Data 0.000 (0.000)	Loss 2.0655 (1.9646)	Prec@1 53.906 (55.385)	Prec@5 79.102 (79.215)
[2025/02/01 00:10:54] - Train Epoch: [84][660/2503]	Time 2.808 (2.816)	Data 0.000 (0.000)	Loss 2.1383 (1.9654)	Prec@1 51.172 (55.367)	Prec@5 77.539 (79.202)
Train Epoch: [84][660/2503]	Time 2.808 (2.816)	Data 0.000 (0.000)	Loss 2.1383 (1.9654)	Prec@1 51.172 (55.367)	Prec@5 77.539 (79.202)
[2025/02/01 00:11:53] - Train Epoch: [84][680/2503]	Time 2.802 (2.815)	Data 0.000 (0.000)	Loss 2.0120 (1.9646)	Prec@1 52.344 (55.382)	Prec@5 77.930 (79.219)
Train Epoch: [84][680/2503]	Time 2.802 (2.815)	Data 0.000 (0.000)	Loss 2.0120 (1.9646)	Prec@1 52.344 (55.382)	Prec@5 77.930 (79.219)
[2025/02/01 00:12:53] - Train Epoch: [84][700/2503]	Time 2.807 (2.815)	Data 0.000 (0.000)	Loss 1.9629 (1.9640)	Prec@1 55.664 (55.407)	Prec@5 78.320 (79.213)
Train Epoch: [84][700/2503]	Time 2.807 (2.815)	Data 0.000 (0.000)	Loss 1.9629 (1.9640)	Prec@1 55.664 (55.407)	Prec@5 78.320 (79.213)
[2025/02/01 00:13:52] - Train Epoch: [84][720/2503]	Time 2.807 (2.815)	Data 0.000 (0.000)	Loss 1.8990 (1.9634)	Prec@1 55.859 (55.425)	Prec@5 80.469 (79.219)
Train Epoch: [84][720/2503]	Time 2.807 (2.815)	Data 0.000 (0.000)	Loss 1.8990 (1.9634)	Prec@1 55.859 (55.425)	Prec@5 80.469 (79.219)
[2025/02/01 00:14:51] - Train Epoch: [84][740/2503]	Time 2.804 (2.815)	Data 0.000 (0.000)	Loss 1.9185 (1.9622)	Prec@1 57.031 (55.442)	Prec@5 79.102 (79.234)
Train Epoch: [84][740/2503]	Time 2.804 (2.815)	Data 0.000 (0.000)	Loss 1.9185 (1.9622)	Prec@1 57.031 (55.442)	Prec@5 79.102 (79.234)
[2025/02/01 00:15:50] - Train Epoch: [84][760/2503]	Time 2.808 (2.815)	Data 0.000 (0.000)	Loss 2.0891 (1.9620)	Prec@1 53.711 (55.443)	Prec@5 78.320 (79.234)
Train Epoch: [84][760/2503]	Time 2.808 (2.815)	Data 0.000 (0.000)	Loss 2.0891 (1.9620)	Prec@1 53.711 (55.443)	Prec@5 78.320 (79.234)
[2025/02/01 00:16:50] - Train Epoch: [84][780/2503]	Time 2.808 (2.815)	Data 0.000 (0.000)	Loss 1.7600 (1.9618)	Prec@1 60.156 (55.453)	Prec@5 80.273 (79.237)
Train Epoch: [84][780/2503]	Time 2.808 (2.815)	Data 0.000 (0.000)	Loss 1.7600 (1.9618)	Prec@1 60.156 (55.453)	Prec@5 80.273 (79.237)
[2025/02/01 00:17:50] - Train Epoch: [84][800/2503]	Time 2.804 (2.815)	Data 0.000 (0.000)	Loss 1.8805 (1.9617)	Prec@1 58.008 (55.451)	Prec@5 80.469 (79.242)
Train Epoch: [84][800/2503]	Time 2.804 (2.815)	Data 0.000 (0.000)	Loss 1.8805 (1.9617)	Prec@1 58.008 (55.451)	Prec@5 80.469 (79.242)
[2025/02/01 00:18:49] - Train Epoch: [84][820/2503]	Time 2.807 (2.814)	Data 0.000 (0.000)	Loss 2.0078 (1.9614)	Prec@1 54.688 (55.453)	Prec@5 79.492 (79.246)
Train Epoch: [84][820/2503]	Time 2.807 (2.814)	Data 0.000 (0.000)	Loss 2.0078 (1.9614)	Prec@1 54.688 (55.453)	Prec@5 79.492 (79.246)
[2025/02/01 00:19:48] - Train Epoch: [84][840/2503]	Time 2.808 (2.814)	Data 0.000 (0.000)	Loss 1.8994 (1.9620)	Prec@1 55.859 (55.439)	Prec@5 80.273 (79.228)
Train Epoch: [84][840/2503]	Time 2.808 (2.814)	Data 0.000 (0.000)	Loss 1.8994 (1.9620)	Prec@1 55.859 (55.439)	Prec@5 80.273 (79.228)
[2025/02/01 00:20:47] - Train Epoch: [84][860/2503]	Time 2.802 (2.814)	Data 0.000 (0.000)	Loss 1.9776 (1.9629)	Prec@1 55.273 (55.422)	Prec@5 79.883 (79.210)
Train Epoch: [84][860/2503]	Time 2.802 (2.814)	Data 0.000 (0.000)	Loss 1.9776 (1.9629)	Prec@1 55.273 (55.422)	Prec@5 79.883 (79.210)
[2025/02/01 00:21:46] - Train Epoch: [84][880/2503]	Time 2.805 (2.814)	Data 0.000 (0.000)	Loss 1.9500 (1.9631)	Prec@1 56.250 (55.423)	Prec@5 79.883 (79.222)
Train Epoch: [84][880/2503]	Time 2.805 (2.814)	Data 0.000 (0.000)	Loss 1.9500 (1.9631)	Prec@1 56.250 (55.423)	Prec@5 79.883 (79.222)
[2025/02/01 00:22:45] - Train Epoch: [84][900/2503]	Time 2.804 (2.814)	Data 0.000 (0.000)	Loss 2.0345 (1.9630)	Prec@1 52.539 (55.432)	Prec@5 78.516 (79.219)
Train Epoch: [84][900/2503]	Time 2.804 (2.814)	Data 0.000 (0.000)	Loss 2.0345 (1.9630)	Prec@1 52.539 (55.432)	Prec@5 78.516 (79.219)
[2025/02/01 00:23:45] - Train Epoch: [84][920/2503]	Time 2.802 (2.814)	Data 0.000 (0.000)	Loss 1.7881 (1.9625)	Prec@1 57.812 (55.445)	Prec@5 81.836 (79.229)
Train Epoch: [84][920/2503]	Time 2.802 (2.814)	Data 0.000 (0.000)	Loss 1.7881 (1.9625)	Prec@1 57.812 (55.445)	Prec@5 81.836 (79.229)
[2025/02/01 00:24:44] - Train Epoch: [84][940/2503]	Time 2.805 (2.813)	Data 0.000 (0.000)	Loss 2.0691 (1.9631)	Prec@1 53.125 (55.420)	Prec@5 79.492 (79.214)
Train Epoch: [84][940/2503]	Time 2.805 (2.813)	Data 0.000 (0.000)	Loss 2.0691 (1.9631)	Prec@1 53.125 (55.420)	Prec@5 79.492 (79.214)
[2025/02/01 00:25:43] - Train Epoch: [84][960/2503]	Time 2.808 (2.813)	Data 0.000 (0.000)	Loss 2.0710 (1.9629)	Prec@1 54.492 (55.419)	Prec@5 79.102 (79.220)
Train Epoch: [84][960/2503]	Time 2.808 (2.813)	Data 0.000 (0.000)	Loss 2.0710 (1.9629)	Prec@1 54.492 (55.419)	Prec@5 79.102 (79.220)
[2025/02/01 00:26:42] - Train Epoch: [84][980/2503]	Time 2.804 (2.813)	Data 0.000 (0.000)	Loss 2.0400 (1.9620)	Prec@1 51.758 (55.434)	Prec@5 78.711 (79.232)
Train Epoch: [84][980/2503]	Time 2.804 (2.813)	Data 0.000 (0.000)	Loss 2.0400 (1.9620)	Prec@1 51.758 (55.434)	Prec@5 78.711 (79.232)
[2025/02/01 00:27:41] - Train Epoch: [84][1000/2503]	Time 2.803 (2.813)	Data 0.000 (0.000)	Loss 1.8466 (1.9621)	Prec@1 59.961 (55.445)	Prec@5 80.859 (79.229)
Train Epoch: [84][1000/2503]	Time 2.803 (2.813)	Data 0.000 (0.000)	Loss 1.8466 (1.9621)	Prec@1 59.961 (55.445)	Prec@5 80.859 (79.229)
[2025/02/01 00:28:40] - Train Epoch: [84][1020/2503]	Time 2.806 (2.813)	Data 0.000 (0.000)	Loss 2.0560 (1.9616)	Prec@1 52.148 (55.448)	Prec@5 76.172 (79.239)
Train Epoch: [84][1020/2503]	Time 2.806 (2.813)	Data 0.000 (0.000)	Loss 2.0560 (1.9616)	Prec@1 52.148 (55.448)	Prec@5 76.172 (79.239)
[2025/02/01 00:29:40] - Train Epoch: [84][1040/2503]	Time 2.811 (2.813)	Data 0.000 (0.000)	Loss 1.9215 (1.9622)	Prec@1 56.250 (55.422)	Prec@5 80.469 (79.227)
Train Epoch: [84][1040/2503]	Time 2.811 (2.813)	Data 0.000 (0.000)	Loss 1.9215 (1.9622)	Prec@1 56.250 (55.422)	Prec@5 80.469 (79.227)
[2025/02/01 00:30:39] - Train Epoch: [84][1060/2503]	Time 2.804 (2.813)	Data 0.000 (0.000)	Loss 2.2447 (1.9635)	Prec@1 48.242 (55.398)	Prec@5 76.172 (79.220)
Train Epoch: [84][1060/2503]	Time 2.804 (2.813)	Data 0.000 (0.000)	Loss 2.2447 (1.9635)	Prec@1 48.242 (55.398)	Prec@5 76.172 (79.220)
[2025/02/01 00:31:38] - Train Epoch: [84][1080/2503]	Time 2.807 (2.813)	Data 0.000 (0.000)	Loss 2.1228 (1.9642)	Prec@1 51.367 (55.387)	Prec@5 76.562 (79.209)
Train Epoch: [84][1080/2503]	Time 2.807 (2.813)	Data 0.000 (0.000)	Loss 2.1228 (1.9642)	Prec@1 51.367 (55.387)	Prec@5 76.562 (79.209)
[2025/02/01 00:32:37] - Train Epoch: [84][1100/2503]	Time 2.806 (2.813)	Data 0.000 (0.000)	Loss 2.0959 (1.9650)	Prec@1 52.930 (55.367)	Prec@5 78.125 (79.196)
Train Epoch: [84][1100/2503]	Time 2.806 (2.813)	Data 0.000 (0.000)	Loss 2.0959 (1.9650)	Prec@1 52.930 (55.367)	Prec@5 78.125 (79.196)
[2025/02/01 00:33:37] - Train Epoch: [84][1120/2503]	Time 2.804 (2.813)	Data 0.000 (0.000)	Loss 1.8846 (1.9649)	Prec@1 56.836 (55.374)	Prec@5 80.664 (79.194)
Train Epoch: [84][1120/2503]	Time 2.804 (2.813)	Data 0.000 (0.000)	Loss 1.8846 (1.9649)	Prec@1 56.836 (55.374)	Prec@5 80.664 (79.194)
[2025/02/01 00:34:36] - Train Epoch: [84][1140/2503]	Time 2.806 (2.813)	Data 0.000 (0.000)	Loss 2.0402 (1.9657)	Prec@1 55.859 (55.353)	Prec@5 77.148 (79.188)
Train Epoch: [84][1140/2503]	Time 2.806 (2.813)	Data 0.000 (0.000)	Loss 2.0402 (1.9657)	Prec@1 55.859 (55.353)	Prec@5 77.148 (79.188)
[2025/02/01 00:35:35] - Train Epoch: [84][1160/2503]	Time 2.806 (2.813)	Data 0.000 (0.000)	Loss 2.1238 (1.9656)	Prec@1 52.344 (55.363)	Prec@5 77.734 (79.183)
Train Epoch: [84][1160/2503]	Time 2.806 (2.813)	Data 0.000 (0.000)	Loss 2.1238 (1.9656)	Prec@1 52.344 (55.363)	Prec@5 77.734 (79.183)
[2025/02/01 00:36:34] - Train Epoch: [84][1180/2503]	Time 2.808 (2.813)	Data 0.000 (0.000)	Loss 1.8631 (1.9660)	Prec@1 58.008 (55.355)	Prec@5 79.102 (79.175)
Train Epoch: [84][1180/2503]	Time 2.808 (2.813)	Data 0.000 (0.000)	Loss 1.8631 (1.9660)	Prec@1 58.008 (55.355)	Prec@5 79.102 (79.175)
[2025/02/01 00:37:34] - Train Epoch: [84][1200/2503]	Time 2.805 (2.812)	Data 0.000 (0.000)	Loss 1.9631 (1.9659)	Prec@1 56.250 (55.361)	Prec@5 78.516 (79.180)
Train Epoch: [84][1200/2503]	Time 2.805 (2.812)	Data 0.000 (0.000)	Loss 1.9631 (1.9659)	Prec@1 56.250 (55.361)	Prec@5 78.516 (79.180)
[2025/02/01 00:38:33] - Train Epoch: [84][1220/2503]	Time 2.804 (2.812)	Data 0.000 (0.000)	Loss 1.9854 (1.9660)	Prec@1 53.125 (55.353)	Prec@5 78.320 (79.181)
Train Epoch: [84][1220/2503]	Time 2.804 (2.812)	Data 0.000 (0.000)	Loss 1.9854 (1.9660)	Prec@1 53.125 (55.353)	Prec@5 78.320 (79.181)
[2025/02/01 00:39:32] - Train Epoch: [84][1240/2503]	Time 2.811 (2.812)	Data 0.000 (0.000)	Loss 1.8895 (1.9664)	Prec@1 58.008 (55.353)	Prec@5 79.883 (79.175)
Train Epoch: [84][1240/2503]	Time 2.811 (2.812)	Data 0.000 (0.000)	Loss 1.8895 (1.9664)	Prec@1 58.008 (55.353)	Prec@5 79.883 (79.175)
[2025/02/01 00:40:31] - Train Epoch: [84][1260/2503]	Time 2.805 (2.812)	Data 0.000 (0.000)	Loss 2.1922 (1.9669)	Prec@1 53.125 (55.346)	Prec@5 75.781 (79.167)
Train Epoch: [84][1260/2503]	Time 2.805 (2.812)	Data 0.000 (0.000)	Loss 2.1922 (1.9669)	Prec@1 53.125 (55.346)	Prec@5 75.781 (79.167)
[2025/02/01 00:41:30] - Train Epoch: [84][1280/2503]	Time 2.808 (2.812)	Data 0.000 (0.000)	Loss 2.1213 (1.9670)	Prec@1 50.977 (55.341)	Prec@5 77.539 (79.164)
Train Epoch: [84][1280/2503]	Time 2.808 (2.812)	Data 0.000 (0.000)	Loss 2.1213 (1.9670)	Prec@1 50.977 (55.341)	Prec@5 77.539 (79.164)
[2025/02/01 00:42:30] - Train Epoch: [84][1300/2503]	Time 2.808 (2.812)	Data 0.000 (0.000)	Loss 1.9150 (1.9671)	Prec@1 57.031 (55.339)	Prec@5 79.883 (79.154)
Train Epoch: [84][1300/2503]	Time 2.808 (2.812)	Data 0.000 (0.000)	Loss 1.9150 (1.9671)	Prec@1 57.031 (55.339)	Prec@5 79.883 (79.154)
[2025/02/01 00:43:29] - Train Epoch: [84][1320/2503]	Time 2.807 (2.812)	Data 0.000 (0.000)	Loss 1.7816 (1.9668)	Prec@1 59.375 (55.343)	Prec@5 81.445 (79.160)
Train Epoch: [84][1320/2503]	Time 2.807 (2.812)	Data 0.000 (0.000)	Loss 1.7816 (1.9668)	Prec@1 59.375 (55.343)	Prec@5 81.445 (79.160)
[2025/02/01 00:44:28] - Train Epoch: [84][1340/2503]	Time 2.814 (2.812)	Data 0.000 (0.000)	Loss 1.8013 (1.9679)	Prec@1 59.180 (55.327)	Prec@5 80.469 (79.145)
Train Epoch: [84][1340/2503]	Time 2.814 (2.812)	Data 0.000 (0.000)	Loss 1.8013 (1.9679)	Prec@1 59.180 (55.327)	Prec@5 80.469 (79.145)
[2025/02/01 00:45:27] - Train Epoch: [84][1360/2503]	Time 2.805 (2.812)	Data 0.000 (0.000)	Loss 1.8521 (1.9679)	Prec@1 59.766 (55.334)	Prec@5 78.906 (79.143)
Train Epoch: [84][1360/2503]	Time 2.805 (2.812)	Data 0.000 (0.000)	Loss 1.8521 (1.9679)	Prec@1 59.766 (55.334)	Prec@5 78.906 (79.143)
[2025/02/01 00:46:26] - Train Epoch: [84][1380/2503]	Time 2.811 (2.812)	Data 0.000 (0.000)	Loss 2.0853 (1.9692)	Prec@1 54.297 (55.306)	Prec@5 77.344 (79.119)
Train Epoch: [84][1380/2503]	Time 2.811 (2.812)	Data 0.000 (0.000)	Loss 2.0853 (1.9692)	Prec@1 54.297 (55.306)	Prec@5 77.344 (79.119)
[2025/02/01 00:47:26] - Train Epoch: [84][1400/2503]	Time 2.807 (2.812)	Data 0.000 (0.000)	Loss 1.8613 (1.9702)	Prec@1 56.445 (55.283)	Prec@5 81.641 (79.109)
Train Epoch: [84][1400/2503]	Time 2.807 (2.812)	Data 0.000 (0.000)	Loss 1.8613 (1.9702)	Prec@1 56.445 (55.283)	Prec@5 81.641 (79.109)
[2025/02/01 00:48:25] - Train Epoch: [84][1420/2503]	Time 2.807 (2.812)	Data 0.000 (0.000)	Loss 1.9219 (1.9701)	Prec@1 58.008 (55.289)	Prec@5 79.883 (79.106)
Train Epoch: [84][1420/2503]	Time 2.807 (2.812)	Data 0.000 (0.000)	Loss 1.9219 (1.9701)	Prec@1 58.008 (55.289)	Prec@5 79.883 (79.106)
[2025/02/01 00:49:24] - Train Epoch: [84][1440/2503]	Time 2.807 (2.812)	Data 0.000 (0.000)	Loss 2.0288 (1.9700)	Prec@1 52.930 (55.289)	Prec@5 77.930 (79.111)
Train Epoch: [84][1440/2503]	Time 2.807 (2.812)	Data 0.000 (0.000)	Loss 2.0288 (1.9700)	Prec@1 52.930 (55.289)	Prec@5 77.930 (79.111)
[2025/02/01 00:50:23] - Train Epoch: [84][1460/2503]	Time 2.804 (2.812)	Data 0.000 (0.000)	Loss 2.0839 (1.9693)	Prec@1 52.539 (55.303)	Prec@5 77.930 (79.120)
Train Epoch: [84][1460/2503]	Time 2.804 (2.812)	Data 0.000 (0.000)	Loss 2.0839 (1.9693)	Prec@1 52.539 (55.303)	Prec@5 77.930 (79.120)
[2025/02/01 00:51:23] - Train Epoch: [84][1480/2503]	Time 2.806 (2.812)	Data 0.000 (0.000)	Loss 2.0065 (1.9695)	Prec@1 52.734 (55.302)	Prec@5 79.492 (79.119)
Train Epoch: [84][1480/2503]	Time 2.806 (2.812)	Data 0.000 (0.000)	Loss 2.0065 (1.9695)	Prec@1 52.734 (55.302)	Prec@5 79.492 (79.119)
[2025/02/01 00:52:22] - Train Epoch: [84][1500/2503]	Time 2.805 (2.812)	Data 0.000 (0.000)	Loss 1.9526 (1.9699)	Prec@1 56.641 (55.301)	Prec@5 80.078 (79.112)
Train Epoch: [84][1500/2503]	Time 2.805 (2.812)	Data 0.000 (0.000)	Loss 1.9526 (1.9699)	Prec@1 56.641 (55.301)	Prec@5 80.078 (79.112)
[2025/02/01 00:53:21] - Train Epoch: [84][1520/2503]	Time 2.808 (2.812)	Data 0.000 (0.000)	Loss 2.0509 (1.9701)	Prec@1 52.930 (55.300)	Prec@5 76.367 (79.102)
Train Epoch: [84][1520/2503]	Time 2.808 (2.812)	Data 0.000 (0.000)	Loss 2.0509 (1.9701)	Prec@1 52.930 (55.300)	Prec@5 76.367 (79.102)
[2025/02/01 00:54:20] - Train Epoch: [84][1540/2503]	Time 2.804 (2.812)	Data 0.000 (0.000)	Loss 1.6932 (1.9698)	Prec@1 59.766 (55.295)	Prec@5 84.961 (79.110)
Train Epoch: [84][1540/2503]	Time 2.804 (2.812)	Data 0.000 (0.000)	Loss 1.6932 (1.9698)	Prec@1 59.766 (55.295)	Prec@5 84.961 (79.110)
[2025/02/01 00:55:19] - Train Epoch: [84][1560/2503]	Time 2.811 (2.812)	Data 0.000 (0.000)	Loss 2.0801 (1.9699)	Prec@1 49.609 (55.289)	Prec@5 78.906 (79.108)
Train Epoch: [84][1560/2503]	Time 2.811 (2.812)	Data 0.000 (0.000)	Loss 2.0801 (1.9699)	Prec@1 49.609 (55.289)	Prec@5 78.906 (79.108)
[2025/02/01 00:56:19] - Train Epoch: [84][1580/2503]	Time 2.804 (2.812)	Data 0.000 (0.000)	Loss 1.9321 (1.9695)	Prec@1 53.320 (55.295)	Prec@5 80.859 (79.114)
Train Epoch: [84][1580/2503]	Time 2.804 (2.812)	Data 0.000 (0.000)	Loss 1.9321 (1.9695)	Prec@1 53.320 (55.295)	Prec@5 80.859 (79.114)
[2025/02/01 00:57:18] - Train Epoch: [84][1600/2503]	Time 2.806 (2.812)	Data 0.000 (0.000)	Loss 1.9753 (1.9689)	Prec@1 55.469 (55.306)	Prec@5 80.078 (79.123)
Train Epoch: [84][1600/2503]	Time 2.806 (2.812)	Data 0.000 (0.000)	Loss 1.9753 (1.9689)	Prec@1 55.469 (55.306)	Prec@5 80.078 (79.123)
[2025/02/01 00:58:17] - Train Epoch: [84][1620/2503]	Time 2.803 (2.811)	Data 0.000 (0.000)	Loss 2.3313 (1.9694)	Prec@1 47.461 (55.290)	Prec@5 74.414 (79.117)
Train Epoch: [84][1620/2503]	Time 2.803 (2.811)	Data 0.000 (0.000)	Loss 2.3313 (1.9694)	Prec@1 47.461 (55.290)	Prec@5 74.414 (79.117)
[2025/02/01 00:59:16] - Train Epoch: [84][1640/2503]	Time 2.806 (2.811)	Data 0.000 (0.000)	Loss 1.8952 (1.9694)	Prec@1 58.789 (55.294)	Prec@5 80.664 (79.118)
Train Epoch: [84][1640/2503]	Time 2.806 (2.811)	Data 0.000 (0.000)	Loss 1.8952 (1.9694)	Prec@1 58.789 (55.294)	Prec@5 80.664 (79.118)
[2025/02/01 01:00:15] - Train Epoch: [84][1660/2503]	Time 2.810 (2.811)	Data 0.000 (0.000)	Loss 1.9850 (1.9695)	Prec@1 55.469 (55.295)	Prec@5 80.078 (79.121)
Train Epoch: [84][1660/2503]	Time 2.810 (2.811)	Data 0.000 (0.000)	Loss 1.9850 (1.9695)	Prec@1 55.469 (55.295)	Prec@5 80.078 (79.121)
[2025/02/01 01:01:14] - Train Epoch: [84][1680/2503]	Time 2.805 (2.811)	Data 0.000 (0.000)	Loss 1.8396 (1.9694)	Prec@1 56.836 (55.290)	Prec@5 81.055 (79.120)
Train Epoch: [84][1680/2503]	Time 2.805 (2.811)	Data 0.000 (0.000)	Loss 1.8396 (1.9694)	Prec@1 56.836 (55.290)	Prec@5 81.055 (79.120)
[2025/02/01 01:02:14] - Train Epoch: [84][1700/2503]	Time 2.804 (2.811)	Data 0.000 (0.000)	Loss 2.0189 (1.9696)	Prec@1 55.078 (55.287)	Prec@5 77.344 (79.117)
Train Epoch: [84][1700/2503]	Time 2.804 (2.811)	Data 0.000 (0.000)	Loss 2.0189 (1.9696)	Prec@1 55.078 (55.287)	Prec@5 77.344 (79.117)
[2025/02/01 01:03:13] - Train Epoch: [84][1720/2503]	Time 2.809 (2.811)	Data 0.000 (0.000)	Loss 2.0872 (1.9697)	Prec@1 55.078 (55.286)	Prec@5 75.977 (79.115)
Train Epoch: [84][1720/2503]	Time 2.809 (2.811)	Data 0.000 (0.000)	Loss 2.0872 (1.9697)	Prec@1 55.078 (55.286)	Prec@5 75.977 (79.115)
[2025/02/01 01:04:12] - Train Epoch: [84][1740/2503]	Time 2.805 (2.811)	Data 0.000 (0.000)	Loss 1.8418 (1.9691)	Prec@1 55.469 (55.291)	Prec@5 81.055 (79.125)
Train Epoch: [84][1740/2503]	Time 2.805 (2.811)	Data 0.000 (0.000)	Loss 1.8418 (1.9691)	Prec@1 55.469 (55.291)	Prec@5 81.055 (79.125)
[2025/02/01 01:05:11] - Train Epoch: [84][1760/2503]	Time 2.803 (2.811)	Data 0.000 (0.000)	Loss 2.2740 (1.9690)	Prec@1 46.875 (55.294)	Prec@5 77.148 (79.126)
Train Epoch: [84][1760/2503]	Time 2.803 (2.811)	Data 0.000 (0.000)	Loss 2.2740 (1.9690)	Prec@1 46.875 (55.294)	Prec@5 77.148 (79.126)
[2025/02/01 01:06:10] - Train Epoch: [84][1780/2503]	Time 2.808 (2.811)	Data 0.000 (0.000)	Loss 2.1027 (1.9688)	Prec@1 55.078 (55.303)	Prec@5 76.953 (79.125)
Train Epoch: [84][1780/2503]	Time 2.808 (2.811)	Data 0.000 (0.000)	Loss 2.1027 (1.9688)	Prec@1 55.078 (55.303)	Prec@5 76.953 (79.125)
[2025/02/01 01:07:10] - Train Epoch: [84][1800/2503]	Time 2.803 (2.811)	Data 0.000 (0.000)	Loss 1.8864 (1.9686)	Prec@1 55.273 (55.298)	Prec@5 81.445 (79.127)
Train Epoch: [84][1800/2503]	Time 2.803 (2.811)	Data 0.000 (0.000)	Loss 1.8864 (1.9686)	Prec@1 55.273 (55.298)	Prec@5 81.445 (79.127)
[2025/02/01 01:08:09] - Train Epoch: [84][1820/2503]	Time 2.805 (2.811)	Data 0.000 (0.000)	Loss 1.9987 (1.9690)	Prec@1 54.102 (55.288)	Prec@5 78.516 (79.123)
Train Epoch: [84][1820/2503]	Time 2.805 (2.811)	Data 0.000 (0.000)	Loss 1.9987 (1.9690)	Prec@1 54.102 (55.288)	Prec@5 78.516 (79.123)
[2025/02/01 01:09:08] - Train Epoch: [84][1840/2503]	Time 2.808 (2.811)	Data 0.000 (0.000)	Loss 2.1572 (1.9686)	Prec@1 49.609 (55.293)	Prec@5 76.562 (79.126)
Train Epoch: [84][1840/2503]	Time 2.808 (2.811)	Data 0.000 (0.000)	Loss 2.1572 (1.9686)	Prec@1 49.609 (55.293)	Prec@5 76.562 (79.126)
[2025/02/01 01:10:07] - Train Epoch: [84][1860/2503]	Time 2.808 (2.811)	Data 0.000 (0.000)	Loss 2.0687 (1.9687)	Prec@1 53.516 (55.292)	Prec@5 78.516 (79.125)
Train Epoch: [84][1860/2503]	Time 2.808 (2.811)	Data 0.000 (0.000)	Loss 2.0687 (1.9687)	Prec@1 53.516 (55.292)	Prec@5 78.516 (79.125)
[2025/02/01 01:11:06] - Train Epoch: [84][1880/2503]	Time 2.805 (2.811)	Data 0.000 (0.000)	Loss 1.8283 (1.9686)	Prec@1 57.617 (55.291)	Prec@5 81.836 (79.126)
Train Epoch: [84][1880/2503]	Time 2.805 (2.811)	Data 0.000 (0.000)	Loss 1.8283 (1.9686)	Prec@1 57.617 (55.291)	Prec@5 81.836 (79.126)
[2025/02/01 01:12:06] - Train Epoch: [84][1900/2503]	Time 2.803 (2.811)	Data 0.000 (0.000)	Loss 2.0206 (1.9685)	Prec@1 53.906 (55.296)	Prec@5 77.930 (79.126)
Train Epoch: [84][1900/2503]	Time 2.803 (2.811)	Data 0.000 (0.000)	Loss 2.0206 (1.9685)	Prec@1 53.906 (55.296)	Prec@5 77.930 (79.126)
[2025/02/01 01:13:05] - Train Epoch: [84][1920/2503]	Time 2.811 (2.811)	Data 0.000 (0.000)	Loss 1.8193 (1.9685)	Prec@1 58.594 (55.296)	Prec@5 82.617 (79.131)
Train Epoch: [84][1920/2503]	Time 2.811 (2.811)	Data 0.000 (0.000)	Loss 1.8193 (1.9685)	Prec@1 58.594 (55.296)	Prec@5 82.617 (79.131)
[2025/02/01 01:14:04] - Train Epoch: [84][1940/2503]	Time 2.806 (2.811)	Data 0.000 (0.000)	Loss 2.1048 (1.9686)	Prec@1 55.664 (55.296)	Prec@5 77.539 (79.128)
Train Epoch: [84][1940/2503]	Time 2.806 (2.811)	Data 0.000 (0.000)	Loss 2.1048 (1.9686)	Prec@1 55.664 (55.296)	Prec@5 77.539 (79.128)
[2025/02/01 01:15:03] - Train Epoch: [84][1960/2503]	Time 2.800 (2.811)	Data 0.000 (0.000)	Loss 1.8628 (1.9683)	Prec@1 57.422 (55.299)	Prec@5 80.469 (79.131)
Train Epoch: [84][1960/2503]	Time 2.800 (2.811)	Data 0.000 (0.000)	Loss 1.8628 (1.9683)	Prec@1 57.422 (55.299)	Prec@5 80.469 (79.131)
[2025/02/01 01:16:02] - Train Epoch: [84][1980/2503]	Time 2.830 (2.811)	Data 0.000 (0.000)	Loss 1.8638 (1.9676)	Prec@1 58.594 (55.313)	Prec@5 82.227 (79.142)
Train Epoch: [84][1980/2503]	Time 2.830 (2.811)	Data 0.000 (0.000)	Loss 1.8638 (1.9676)	Prec@1 58.594 (55.313)	Prec@5 82.227 (79.142)
[2025/02/01 01:17:02] - Train Epoch: [84][2000/2503]	Time 2.803 (2.811)	Data 0.000 (0.000)	Loss 1.8453 (1.9674)	Prec@1 59.375 (55.316)	Prec@5 80.469 (79.147)
Train Epoch: [84][2000/2503]	Time 2.803 (2.811)	Data 0.000 (0.000)	Loss 1.8453 (1.9674)	Prec@1 59.375 (55.316)	Prec@5 80.469 (79.147)
[2025/02/01 01:18:01] - Train Epoch: [84][2020/2503]	Time 2.806 (2.811)	Data 0.000 (0.000)	Loss 1.6702 (1.9669)	Prec@1 63.672 (55.329)	Prec@5 84.570 (79.158)
Train Epoch: [84][2020/2503]	Time 2.806 (2.811)	Data 0.000 (0.000)	Loss 1.6702 (1.9669)	Prec@1 63.672 (55.329)	Prec@5 84.570 (79.158)
[2025/02/01 01:19:00] - Train Epoch: [84][2040/2503]	Time 2.805 (2.811)	Data 0.000 (0.000)	Loss 1.8168 (1.9668)	Prec@1 59.961 (55.333)	Prec@5 80.469 (79.160)
Train Epoch: [84][2040/2503]	Time 2.805 (2.811)	Data 0.000 (0.000)	Loss 1.8168 (1.9668)	Prec@1 59.961 (55.333)	Prec@5 80.469 (79.160)
[2025/02/01 01:19:59] - Train Epoch: [84][2060/2503]	Time 2.805 (2.811)	Data 0.000 (0.000)	Loss 2.0060 (1.9669)	Prec@1 56.250 (55.330)	Prec@5 76.562 (79.157)
Train Epoch: [84][2060/2503]	Time 2.805 (2.811)	Data 0.000 (0.000)	Loss 2.0060 (1.9669)	Prec@1 56.250 (55.330)	Prec@5 76.562 (79.157)
[2025/02/01 01:20:58] - Train Epoch: [84][2080/2503]	Time 2.806 (2.811)	Data 0.000 (0.000)	Loss 2.0426 (1.9667)	Prec@1 55.078 (55.343)	Prec@5 77.148 (79.153)
Train Epoch: [84][2080/2503]	Time 2.806 (2.811)	Data 0.000 (0.000)	Loss 2.0426 (1.9667)	Prec@1 55.078 (55.343)	Prec@5 77.148 (79.153)
[2025/02/01 01:21:58] - Train Epoch: [84][2100/2503]	Time 2.804 (2.811)	Data 0.000 (0.000)	Loss 2.1557 (1.9668)	Prec@1 50.391 (55.347)	Prec@5 75.781 (79.147)
Train Epoch: [84][2100/2503]	Time 2.804 (2.811)	Data 0.000 (0.000)	Loss 2.1557 (1.9668)	Prec@1 50.391 (55.347)	Prec@5 75.781 (79.147)
[2025/02/01 01:22:57] - Train Epoch: [84][2120/2503]	Time 2.808 (2.811)	Data 0.000 (0.000)	Loss 1.9312 (1.9664)	Prec@1 57.422 (55.348)	Prec@5 79.492 (79.151)
Train Epoch: [84][2120/2503]	Time 2.808 (2.811)	Data 0.000 (0.000)	Loss 1.9312 (1.9664)	Prec@1 57.422 (55.348)	Prec@5 79.492 (79.151)
[2025/02/01 01:23:56] - Train Epoch: [84][2140/2503]	Time 2.804 (2.811)	Data 0.000 (0.000)	Loss 2.0664 (1.9662)	Prec@1 52.148 (55.347)	Prec@5 77.344 (79.153)
Train Epoch: [84][2140/2503]	Time 2.804 (2.811)	Data 0.000 (0.000)	Loss 2.0664 (1.9662)	Prec@1 52.148 (55.347)	Prec@5 77.344 (79.153)
[2025/02/01 01:24:55] - Train Epoch: [84][2160/2503]	Time 2.805 (2.811)	Data 0.000 (0.000)	Loss 1.7269 (1.9665)	Prec@1 60.742 (55.344)	Prec@5 80.859 (79.147)
Train Epoch: [84][2160/2503]	Time 2.805 (2.811)	Data 0.000 (0.000)	Loss 1.7269 (1.9665)	Prec@1 60.742 (55.344)	Prec@5 80.859 (79.147)
[2025/02/01 01:25:55] - Train Epoch: [84][2180/2503]	Time 2.805 (2.811)	Data 0.000 (0.000)	Loss 2.0923 (1.9665)	Prec@1 53.516 (55.344)	Prec@5 77.734 (79.149)
Train Epoch: [84][2180/2503]	Time 2.805 (2.811)	Data 0.000 (0.000)	Loss 2.0923 (1.9665)	Prec@1 53.516 (55.344)	Prec@5 77.734 (79.149)
[2025/02/01 01:26:54] - Train Epoch: [84][2200/2503]	Time 2.809 (2.811)	Data 0.000 (0.000)	Loss 2.0520 (1.9670)	Prec@1 53.125 (55.333)	Prec@5 77.539 (79.142)
Train Epoch: [84][2200/2503]	Time 2.809 (2.811)	Data 0.000 (0.000)	Loss 2.0520 (1.9670)	Prec@1 53.125 (55.333)	Prec@5 77.539 (79.142)
[2025/02/01 01:27:53] - Train Epoch: [84][2220/2503]	Time 2.810 (2.811)	Data 0.000 (0.000)	Loss 1.9873 (1.9665)	Prec@1 56.445 (55.344)	Prec@5 77.539 (79.149)
Train Epoch: [84][2220/2503]	Time 2.810 (2.811)	Data 0.000 (0.000)	Loss 1.9873 (1.9665)	Prec@1 56.445 (55.344)	Prec@5 77.539 (79.149)
[2025/02/01 01:28:52] - Train Epoch: [84][2240/2503]	Time 2.806 (2.811)	Data 0.000 (0.000)	Loss 2.0720 (1.9672)	Prec@1 52.344 (55.332)	Prec@5 76.758 (79.138)
Train Epoch: [84][2240/2503]	Time 2.806 (2.811)	Data 0.000 (0.000)	Loss 2.0720 (1.9672)	Prec@1 52.344 (55.332)	Prec@5 76.758 (79.138)
[2025/02/01 01:29:51] - Train Epoch: [84][2260/2503]	Time 2.803 (2.811)	Data 0.000 (0.000)	Loss 1.9494 (1.9671)	Prec@1 56.445 (55.340)	Prec@5 79.492 (79.139)
Train Epoch: [84][2260/2503]	Time 2.803 (2.811)	Data 0.000 (0.000)	Loss 1.9494 (1.9671)	Prec@1 56.445 (55.340)	Prec@5 79.492 (79.139)
[2025/02/01 01:30:51] - Train Epoch: [84][2280/2503]	Time 2.807 (2.811)	Data 0.000 (0.000)	Loss 1.7429 (1.9671)	Prec@1 62.500 (55.339)	Prec@5 81.641 (79.136)
Train Epoch: [84][2280/2503]	Time 2.807 (2.811)	Data 0.000 (0.000)	Loss 1.7429 (1.9671)	Prec@1 62.500 (55.339)	Prec@5 81.641 (79.136)
[2025/02/01 01:31:50] - Train Epoch: [84][2300/2503]	Time 2.812 (2.811)	Data 0.000 (0.000)	Loss 2.0051 (1.9669)	Prec@1 54.102 (55.340)	Prec@5 76.172 (79.138)
Train Epoch: [84][2300/2503]	Time 2.812 (2.811)	Data 0.000 (0.000)	Loss 2.0051 (1.9669)	Prec@1 54.102 (55.340)	Prec@5 76.172 (79.138)
[2025/02/01 01:32:49] - Train Epoch: [84][2320/2503]	Time 2.806 (2.811)	Data 0.000 (0.000)	Loss 1.9766 (1.9669)	Prec@1 55.078 (55.342)	Prec@5 77.539 (79.138)
Train Epoch: [84][2320/2503]	Time 2.806 (2.811)	Data 0.000 (0.000)	Loss 1.9766 (1.9669)	Prec@1 55.078 (55.342)	Prec@5 77.539 (79.138)
[2025/02/01 01:33:48] - Train Epoch: [84][2340/2503]	Time 2.810 (2.811)	Data 0.000 (0.000)	Loss 1.8493 (1.9668)	Prec@1 58.398 (55.342)	Prec@5 80.078 (79.136)
Train Epoch: [84][2340/2503]	Time 2.810 (2.811)	Data 0.000 (0.000)	Loss 1.8493 (1.9668)	Prec@1 58.398 (55.342)	Prec@5 80.078 (79.136)
[2025/02/01 01:34:47] - Train Epoch: [84][2360/2503]	Time 2.806 (2.811)	Data 0.000 (0.000)	Loss 1.9302 (1.9663)	Prec@1 56.445 (55.356)	Prec@5 79.492 (79.139)
Train Epoch: [84][2360/2503]	Time 2.806 (2.811)	Data 0.000 (0.000)	Loss 1.9302 (1.9663)	Prec@1 56.445 (55.356)	Prec@5 79.492 (79.139)
[2025/02/01 01:35:46] - Train Epoch: [84][2380/2503]	Time 2.803 (2.811)	Data 0.000 (0.000)	Loss 1.7363 (1.9662)	Prec@1 58.203 (55.356)	Prec@5 81.836 (79.140)
Train Epoch: [84][2380/2503]	Time 2.803 (2.811)	Data 0.000 (0.000)	Loss 1.7363 (1.9662)	Prec@1 58.203 (55.356)	Prec@5 81.836 (79.140)
[2025/02/01 01:36:46] - Train Epoch: [84][2400/2503]	Time 2.808 (2.811)	Data 0.000 (0.000)	Loss 2.0515 (1.9657)	Prec@1 53.516 (55.361)	Prec@5 77.930 (79.148)
Train Epoch: [84][2400/2503]	Time 2.808 (2.811)	Data 0.000 (0.000)	Loss 2.0515 (1.9657)	Prec@1 53.516 (55.361)	Prec@5 77.930 (79.148)
[2025/02/01 01:37:45] - Train Epoch: [84][2420/2503]	Time 2.805 (2.811)	Data 0.000 (0.000)	Loss 1.8933 (1.9658)	Prec@1 56.641 (55.363)	Prec@5 79.688 (79.146)
Train Epoch: [84][2420/2503]	Time 2.805 (2.811)	Data 0.000 (0.000)	Loss 1.8933 (1.9658)	Prec@1 56.641 (55.363)	Prec@5 79.688 (79.146)
[2025/02/01 01:38:44] - Train Epoch: [84][2440/2503]	Time 2.812 (2.811)	Data 0.000 (0.000)	Loss 1.7872 (1.9656)	Prec@1 59.180 (55.368)	Prec@5 78.711 (79.147)
Train Epoch: [84][2440/2503]	Time 2.812 (2.811)	Data 0.000 (0.000)	Loss 1.7872 (1.9656)	Prec@1 59.180 (55.368)	Prec@5 78.711 (79.147)
[2025/02/01 01:39:48] - Train Epoch: [84][2460/2503]	Time 2.802 (2.811)	Data 0.000 (0.000)	Loss 1.9834 (1.9654)	Prec@1 57.227 (55.374)	Prec@5 78.711 (79.150)
Train Epoch: [84][2460/2503]	Time 2.802 (2.811)	Data 0.000 (0.000)	Loss 1.9834 (1.9654)	Prec@1 57.227 (55.374)	Prec@5 78.711 (79.150)
[2025/02/01 01:40:47] - Train Epoch: [84][2480/2503]	Time 2.805 (2.811)	Data 0.000 (0.000)	Loss 1.9268 (1.9655)	Prec@1 58.203 (55.370)	Prec@5 80.273 (79.147)
Train Epoch: [84][2480/2503]	Time 2.805 (2.811)	Data 0.000 (0.000)	Loss 1.9268 (1.9655)	Prec@1 58.203 (55.370)	Prec@5 80.273 (79.147)
[2025/02/01 01:41:46] - Train Epoch: [84][2500/2503]	Time 2.806 (2.811)	Data 0.000 (0.000)	Loss 1.8382 (1.9653)	Prec@1 56.055 (55.370)	Prec@5 82.227 (79.151)
Train Epoch: [84][2500/2503]	Time 2.806 (2.811)	Data 0.000 (0.000)	Loss 1.8382 (1.9653)	Prec@1 56.055 (55.370)	Prec@5 82.227 (79.151)
[2025/02/01 01:41:54] - *** PGD eps: 0.01568627450980392, K: 10, step: 0.00392156862745098 ***
*** PGD eps: 0.01568627450980392, K: 10, step: 0.00392156862745098 ***
[2025/02/01 01:42:00] - PGD Test: [0/98]	Time 5.651 (5.651)	Loss 1.3773 (1.3773)	Prec@1 63.672 (63.672)	Prec@5 87.891 (87.891)
PGD Test: [0/98]	Time 5.651 (5.651)	Loss 1.3773 (1.3773)	Prec@1 63.672 (63.672)	Prec@5 87.891 (87.891)
[2025/02/01 01:42:46] - PGD Test: [20/98]	Time 2.290 (2.454)	Loss 2.0362 (1.8319)	Prec@1 40.430 (54.381)	Prec@5 83.984 (82.264)
PGD Test: [20/98]	Time 2.290 (2.454)	Loss 2.0362 (1.8319)	Prec@1 40.430 (54.381)	Prec@5 83.984 (82.264)
[2025/02/01 01:43:32] - PGD Test: [40/98]	Time 2.292 (2.375)	Loss 2.5645 (1.8164)	Prec@1 42.383 (54.435)	Prec@5 73.047 (83.232)
PGD Test: [40/98]	Time 2.292 (2.375)	Loss 2.5645 (1.8164)	Prec@1 42.383 (54.435)	Prec@5 73.047 (83.232)
[2025/02/01 01:44:18] - PGD Test: [60/98]	Time 2.349 (2.349)	Loss 2.7034 (1.9648)	Prec@1 40.234 (52.894)	Prec@5 68.750 (80.722)
PGD Test: [60/98]	Time 2.349 (2.349)	Loss 2.7034 (1.9648)	Prec@1 40.234 (52.894)	Prec@5 68.750 (80.722)
[2025/02/01 01:45:04] - PGD Test: [80/98]	Time 2.289 (2.334)	Loss 2.2394 (2.0761)	Prec@1 47.461 (51.430)	Prec@5 75.000 (79.017)
PGD Test: [80/98]	Time 2.289 (2.334)	Loss 2.2394 (2.0761)	Prec@1 47.461 (51.430)	Prec@5 75.000 (79.017)
[2025/02/01 01:45:42] -  PGD Final Prec@1 50.850 Prec@5 78.530
 PGD Final Prec@1 50.850 Prec@5 78.530
[2025/02/01 01:45:45] - Test: [0/98]	Time 3.442 (3.442)	Loss 0.8224 (0.8224)	Prec@1 80.273 (80.273)	Prec@5 94.336 (94.336)
Test: [0/98]	Time 3.442 (3.442)	Loss 0.8224 (0.8224)	Prec@1 80.273 (80.273)	Prec@5 94.336 (94.336)
[2025/02/01 01:45:59] - Test: [20/98]	Time 2.241 (0.808)	Loss 1.1385 (1.0124)	Prec@1 69.531 (74.684)	Prec@5 93.164 (91.750)
Test: [20/98]	Time 2.241 (0.808)	Loss 1.1385 (1.0124)	Prec@1 69.531 (74.684)	Prec@5 93.164 (91.750)
[2025/02/01 01:46:13] - Test: [40/98]	Time 2.224 (0.750)	Loss 1.8946 (1.0190)	Prec@1 55.078 (74.347)	Prec@5 81.250 (92.006)
Test: [40/98]	Time 2.224 (0.750)	Loss 1.8946 (1.0190)	Prec@1 55.078 (74.347)	Prec@5 81.250 (92.006)
[2025/02/01 01:46:26] - Test: [60/98]	Time 1.457 (0.719)	Loss 1.9717 (1.1762)	Prec@1 54.492 (71.331)	Prec@5 79.492 (89.802)
Test: [60/98]	Time 1.457 (0.719)	Loss 1.9717 (1.1762)	Prec@1 54.492 (71.331)	Prec@5 79.492 (89.802)
[2025/02/01 01:46:39] - Test: [80/98]	Time 1.286 (0.702)	Loss 1.6153 (1.2746)	Prec@1 64.062 (69.507)	Prec@5 83.594 (88.370)
Test: [80/98]	Time 1.286 (0.702)	Loss 1.6153 (1.2746)	Prec@1 64.062 (69.507)	Prec@5 83.594 (88.370)
[2025/02/01 01:46:50] -  Final Prec@1 68.754 Prec@5 87.978
 Final Prec@1 68.754 Prec@5 87.978
Traceback (most recent call last):
  File "/home/yxma/hzx/NIPS_rand_adv/train_rand_mask_imagenet.py", line 531, in <module>
    main()
  File "/home/yxma/hzx/NIPS_rand_adv/train_rand_mask_imagenet.py", line 337, in main
    ' with K10 Acc: {:.3f}, with K50 Acc: {:.3f}'.format(best_pgd_acc, best_epoch, standard_acc_at_best_pgd, best_pgd_acc_k10, best_pgd_acc_k50))
UnboundLocalError: local variable 'best_pgd_acc_k10' referenced before assignment
wandb: - 1.072 MB of 1.072 MB uploadedwandb: \ 1.072 MB of 1.167 MB uploadedwandb: | 1.162 MB of 1.167 MB uploadedwandb: / 1.162 MB of 1.167 MB uploadedwandb: - 1.162 MB of 1.167 MB uploadedwandb: \ 1.167 MB of 1.167 MB uploadedwandb: 
wandb: Run history:
wandb:   AA Mask Test Loss ‚ñÉ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: AA Mask Test Prec@1 ‚ñÜ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: AA Mask Test Prec@5 ‚ñà‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       PGD Test Loss ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà‚ñÅ‚ñÜ‚ñÖ‚ñá‚ñà
wandb:     PGD Test Prec@1 ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÅ
wandb:     PGD Test Prec@5 ‚ñÜ‚ñÑ‚ñÖ‚ñÇ‚ñÅ‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÅ
wandb:              epoch3 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:          train_acc1 ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:          train_acc5 ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:          train_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   AA Mask Test Loss 1.34319
wandb: AA Mask Test Prec@1 68.108
wandb: AA Mask Test Prec@5 87.456
wandb:       PGD Test Loss 2.07606
wandb:     PGD Test Prec@1 51.42988
wandb:     PGD Test Prec@5 79.01717
wandb:              epoch3 84
wandb:          train_acc1 55.37006
wandb:          train_acc5 79.15078
wandb:          train_loss 1.96533
wandb: 
wandb: üöÄ View run fearless-tree-81 at: https://wandb.ai/xuanzhu_07-university-of-sydney/hole_imagenet/runs/poqonyys
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/xuanzhu_07-university-of-sydney/hole_imagenet
wandb: Synced 6 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250131_064102-poqonyys/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
